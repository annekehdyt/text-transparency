{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reference : \n",
    "# https://richliao.github.io/supervised/classification/2016/11/26/textclassifier-convolutional/\n",
    "\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# os.environ['THEANO_FLAGS'] = \"device=cuda*\"\n",
    "os.environ['KERAS_BACKEND']='tensorflow'\n",
    "# os.environ['MKL_THREADING_LAYER']='GNU'\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer, text_to_word_sequence\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Dense, Input, Flatten\n",
    "from keras.layers import Conv1D, MaxPooling1D, Embedding, Dropout, LSTM, GRU, Bidirectional, TimeDistributed\n",
    "# Merge\n",
    "from keras.models import Model\n",
    "\n",
    "from keras import backend as K\n",
    "from keras.engine.topology import Layer, InputSpec\n",
    "# from keras import initializations\n",
    "from keras import initializers\n",
    "from keras import regularizers\n",
    "from keras import optimizers\n",
    "from keras import constraints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SENT_LENGTH = 100\n",
    "MAX_SENTS = 15\n",
    "MAX_NB_WORDS = 20000\n",
    "EMBEDDING_DIM = 100\n",
    "VALIDATION_SPLIT = 0.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IMDB load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "def save_pickle(path, X):\n",
    "    with open(path, 'wb') as f:\n",
    "        pickle.dump(X, f)\n",
    "\n",
    "def open_pickle(path):\n",
    "    with open(path, 'rb') as f:\n",
    "        X = pickle.load(f)\n",
    "    return X\n",
    "\n",
    "X_train = open_pickle(\"../../data/imdb/imdb_original_preprocessed_xtrain.pickle\")\n",
    "X_test = open_pickle(\"../../data/imdb/imdb_original_preprocessed_xtest.pickle\")\n",
    "y_train = open_pickle(\"../../data/imdb/imdb_original_preprocessed_ytrain.pickle\")\n",
    "y_test = open_pickle(\"../../data/imdb/imdb_original_preprocessed_ytest.pickle\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Yelp Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "\n",
    "def parse(path):\n",
    "    g = gzip.open(path, 'r')\n",
    "    for l in g:\n",
    "        yield eval(l)\n",
    "        \n",
    "def extract(path, key):\n",
    "    corpus = []\n",
    "    y = []\n",
    "    text = parse(path)\n",
    "    for l in text:\n",
    "        corpus.append(l[key])\n",
    "        y.append(l['overall'])\n",
    "    return corpus, y\n",
    "\n",
    "path = r\"..\\..\\data\\reviews_Amazon_Instant_Video_5.json.gz\"\n",
    "X, y = extract(path, 'reviewText')\n",
    "\n",
    "y_norm = []\n",
    "\n",
    "for target in y:\n",
    "    if target>2.5:\n",
    "        y_norm.append(1)\n",
    "    else:\n",
    "        y_norm.append(0)\n",
    "        \n",
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_norm, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import tokenize\n",
    "from textblob import TextBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews = [] #sentences\n",
    "for i in range(len(X_train)):\n",
    "    sentences = TextBlob(X_train[i]).raw_sentences\n",
    "    reviews.append(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Anneke Hidayat\\Anaconda3\\lib\\site-packages\\keras_preprocessing\\text.py:174: UserWarning: The `nb_words` argument in `Tokenizer` has been renamed `num_words`.\n",
      "  warnings.warn('The `nb_words` argument in `Tokenizer` '\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer(nb_words=MAX_NB_WORDS)\n",
    "tokenizer.fit_on_texts(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.zeros((len(X_train), MAX_SENTS, MAX_SENT_LENGTH), dtype='int32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, sentences in enumerate(reviews):\n",
    "    for j, sent in enumerate(sentences):\n",
    "        if j<MAX_SENTS:\n",
    "            wordTokens = text_to_word_sequence(sent)\n",
    "            k=0\n",
    "            for _, word in enumerate(wordTokens):\n",
    "                if k<MAX_SENT_LENGTH and tokenizer.word_index[word]<MAX_NB_WORDS:\n",
    "                    data[i,j,k] = tokenizer.word_index[word]\n",
    "                    k=k+1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_index = tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = to_categorical(np.asarray(y_train))\n",
    "labels = np.asarray(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total 49928 unique tokens.\n",
      "Shape of data tensor: (24874, 15, 100)\n",
      "Shape of label tensor: (24874,)\n"
     ]
    }
   ],
   "source": [
    "print('Total %s unique tokens.' % len(word_index))\n",
    "print('Shape of data tensor:', data.shape)\n",
    "print('Shape of label tensor:', labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = np.arange(data.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "data = data[indices]\n",
    "labels = labels[indices]\n",
    "nb_validation_samples = int(VALIDATION_SPLIT * data.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = data[:-nb_validation_samples]\n",
    "y_train = labels[:-nb_validation_samples]\n",
    "x_val = data[-nb_validation_samples:]\n",
    "y_val = labels[-nb_validation_samples:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of positive and negative reviews in training and validation set\n",
      "17948\n",
      "4494\n"
     ]
    }
   ],
   "source": [
    "print('Number of positive and negative reviews in training and validation set')\n",
    "print(y_train.sum(axis=0))\n",
    "print(y_val.sum(axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [],
   "source": [
    "GLOVE_DIR = \"../../data/glove.6B\"\n",
    "embeddings_index = {}\n",
    "f = open(os.path.join(GLOVE_DIR, 'glove.6B.100d.txt'), 'rb')\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "print('Total %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix = np.random.random((len(word_index) + 1, EMBEDDING_DIM))\n",
    "\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "    \n",
    "embedding_layer = Embedding(len(word_index)+1,\n",
    "                               EMBEDDING_DIM,\n",
    "                               weights=[embedding_matrix],\n",
    "                               input_length=MAX_SENT_LENGTH,\n",
    "                               trainable=True)\n",
    "\n",
    "sentence_input = Input(shape=(MAX_SENT_LENGTH,), dtype='int32')\n",
    "embedded_sequences = embedding_layer(sentence_input)\n",
    "l_lstm = Bidirectional(LSTM(100))(embedded_sequences)\n",
    "sentEncoder = Model(sentence_input, l_lstm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_input = Input(shape=(MAX_SENTS, MAX_SENT_LENGTH), dtype='int32')\n",
    "review_encoder = TimeDistributed(sentEncoder)(review_input)\n",
    "l_lstm_sent = Bidirectional(LSTM(100))(review_encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = Dense(1, activation='sigmoid')(l_lstm_sent)\n",
    "model = Model(review_input, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model fitting - Hierarchical LSTM\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_27 (InputLayer)        (None, 15, 100)           0         \n",
      "_________________________________________________________________\n",
      "time_distributed_21 (TimeDis (None, 15, 200)           5153700   \n",
      "_________________________________________________________________\n",
      "bidirectional_23 (Bidirectio (None, 200)               240800    \n",
      "_________________________________________________________________\n",
      "dense_32 (Dense)             (None, 1)                 201       \n",
      "=================================================================\n",
      "Total params: 5,394,701\n",
      "Trainable params: 5,394,701\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(\"model fitting - Hierarchical LSTM\")\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttLayer(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        self.init = initializers.get('normal')\n",
    "        super(AttLayer, self).__init__(**kwargs)\n",
    "            \n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape)==3\n",
    "        self.W = self.init((input_shape[-1],))\n",
    "        self.trainable_weights = [self.W]\n",
    "        super(AttLayer, self).build(input_shape)\n",
    "        \n",
    "    def call(self, x, mask=None):\n",
    "        # u_{it}\n",
    "        eij = K.tanh(K.dot(x, self.W))\n",
    "\n",
    "        # alpha\n",
    "        ai = K.exp(eij)\n",
    "        weights = ai/K.sum(ai,axis=1).dimshuffle(0,'x')\n",
    "        \n",
    "        # s_i\n",
    "        weighted_input = x * weights.dimshuffle(0,1, 'x')\n",
    "        return weighted_input.sum(axis=1)\n",
    "    \n",
    "    def get_output_shape_for(self, input_shape):\n",
    "        return (input_shape[0], input_shape[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://gist.github.com/cbaziotis/7ef97ccf71cbc14366835198c09809d2\n",
    "# other reference : \n",
    "# https://gist.github.com/cbaziotis/6428df359af27d58078ca5ed9792bd6d\n",
    "\n",
    "def dot_product(x, kernel):\n",
    "    if K.backend() == 'tensorflow':\n",
    "        return K.squeeze(K.dot(x, K.expand_dims(kernel)), axis=-1)\n",
    "    else:\n",
    "        return K.dot(x, kernel)\n",
    "    \n",
    "class AttentionWithContext(Layer):\n",
    "    def __init__(self,\n",
    "        W_regularizer=None, u_regularizer=None, b_regularizer=None,\n",
    "        W_constraint=None, u_constraint=None, b_constraint=None,\n",
    "        bias=True, **kwargs):\n",
    "            \n",
    "        self.supports_masking = False\n",
    "        self.init = initializers.get('normal')\n",
    "            \n",
    "        self.W_regularizer = regularizers.get(W_regularizer)\n",
    "        self.u_regularizer = regularizers.get(u_regularizer)\n",
    "        self.b_regularizer = regularizers.get(b_regularizer)\n",
    "            \n",
    "        self.W_constraint = constraints.get(W_constraint)\n",
    "        self.u_constraint = constraints.get(u_constraint)\n",
    "        self.b_constraint = constraints.get(b_constraint)\n",
    "    \n",
    "        self.bias = bias\n",
    "        super(AttentionWithContext, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape) == 3\n",
    "\n",
    "        self.W = self.add_weight((input_shape[-1], input_shape[-1],),\n",
    "                                initializer=self.init,\n",
    "                                name='{}_W'.format(self.name),\n",
    "                                regularizer=self.W_regularizer,\n",
    "                                constraint=self.W_constraint)\n",
    "            \n",
    "        if self.bias:\n",
    "            self.b = self.add_weight((input_shape[-1],),\n",
    "                                    initializer='zero',\n",
    "                                    name='{}_b'.format(self.name),\n",
    "                                    regularizer=self.b_regularizer,\n",
    "                                    constraint=self.b_constraint)\n",
    "            \n",
    "        self.u = self.add_weight((input_shape[-1],),\n",
    "                                initializer=self.init,\n",
    "                                name='{}_u'.format(self.name),\n",
    "                                regularizer=self.u_regularizer,\n",
    "                                constraint=self.u_constraint)\n",
    "    \n",
    "        super(AttentionWithContext, self).build(input_shape)\n",
    "    \n",
    "    def compute_mask(self, input, input_mask=None):\n",
    "        return None\n",
    "        \n",
    "    def call(self, x, mask=None):\n",
    "        uit = dot_product(x, self.W)\n",
    "            \n",
    "        if self.bias:\n",
    "            uit += self.b\n",
    "                \n",
    "        uit = K.tanh(uit)\n",
    "#         ait = K.dot(uit, self.u) # only works on  \n",
    "        \n",
    "        ait = dot_product(uit, self.u)\n",
    "        a = K.exp(ait)\n",
    "        \n",
    "        a /= K.cast(K.sum(a, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n",
    "            \n",
    "        a = K.expand_dims(a)\n",
    "        weighted_input = x * a\n",
    "            \n",
    "        return K.sum(weighted_input,axis=1)\n",
    "        \n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape[0], input_shape[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_input = Input(shape=(MAX_SENT_LENGTH,), dtype='int32')\n",
    "embedded_sequences = embedding_layer(sentence_input)\n",
    "\n",
    "l_gru = Bidirectional(GRU(50, return_sequences=True))(embedded_sequences)\n",
    "l_dense = TimeDistributed(Dense(10))(l_gru)\n",
    "l_att = AttentionWithContext()(l_dense)\n",
    "\n",
    "sentEncoder = Model(sentence_input, l_att)\n",
    "\n",
    "review_input = Input(shape=(MAX_SENTS, MAX_SENT_LENGTH), dtype='int32')\n",
    "review_encoder = TimeDistributed(sentEncoder)(review_input)\n",
    "\n",
    "l_gru_sent = Bidirectional(GRU(50, return_sequences=True))(review_encoder)\n",
    "l_dense_sent = TimeDistributed(Dense(10))(l_gru_sent)\n",
    "l_att_sent = AttentionWithContext()(l_dense_sent)\n",
    "\n",
    "preds = Dense(1, activation='sigmoid')(l_att_sent)\n",
    "model = Model(review_input, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='binary_crossentropy',\n",
    "            optimizer='rmsprop',\n",
    "            metrics=['acc'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_29 (InputLayer)        (None, 15, 100)           0         \n",
      "_________________________________________________________________\n",
      "time_distributed_23 (TimeDis (None, 15, 10)            5039330   \n",
      "_________________________________________________________________\n",
      "bidirectional_25 (Bidirectio (None, 15, 100)           18300     \n",
      "_________________________________________________________________\n",
      "time_distributed_24 (TimeDis (None, 15, 10)            1010      \n",
      "_________________________________________________________________\n",
      "attention_with_context_18 (A (None, 10)                120       \n",
      "_________________________________________________________________\n",
      "dense_35 (Dense)             (None, 1)                 11        \n",
      "=================================================================\n",
      "Total params: 5,058,771\n",
      "Trainable params: 5,058,771\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model fitting - Hierarchical attention network\n",
      "Train on 19900 samples, validate on 4974 samples\n",
      "Epoch 1/1\n",
      " 6750/19900 [=========>....................] - ETA: 14:43 - loss: 0.3271 - acc: 0.9022"
     ]
    }
   ],
   "source": [
    "print('model fitting - Hierarchical attention network')\n",
    "model.fit(x_train, y_train, validation_data=(x_val, y_val),\n",
    "           epochs=1, batch_size=50, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Relevance sentence Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess\n",
    "# path = r\"C:\\Users\\Anneke\\Documents\\GitHub\\data\\imdb-sentence\"\n",
    "path = r\"C:\\Users\\Anneke Hidayat\\Documents\\GitHub\\data\\imdb-sentence\"\n",
    "X_train_sent = open_pickle(path + r\"\\imdb_sentence_xtrain.pickle\")\n",
    "X_test_sent = open_pickle(path + r\"\\imdb_sentence_xtest.pickle\")\n",
    "y_train_sent = open_pickle(path + r\"\\imdb_sentence_ytrain.pickle\")\n",
    "y_test_sent = open_pickle(path + r\"\\imdb_sentence_ytest.pickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SEQUENCE = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.zeros((len(X_train_sent), MAX_SEQUENCE), dtype='int32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Anneke Hidayat\\Anaconda3\\lib\\site-packages\\keras_preprocessing\\text.py:174: UserWarning: The `nb_words` argument in `Tokenizer` has been renamed `num_words`.\n",
      "  warnings.warn('The `nb_words` argument in `Tokenizer` '\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer(nb_words=MAX_NB_WORDS)\n",
    "tokenizer.fit_on_texts(X_train_sent)\n",
    "word_index = tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, doc in enumerate(X_train_sent):\n",
    "    wordTokens = text_to_word_sequence(doc)\n",
    "    for j, word in enumerate(wordTokens):\n",
    "        try:\n",
    "            if j<MAX_SEQUENCE and tokenizer.word_index[word]<MAX_NB_WORDS:\n",
    "                data[i,j] = tokenizer.word_index[word]\n",
    "        except KeyError as error:\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = np.zeros((len(X_test_sent), MAX_SEQUENCE), dtype='int32')\n",
    "for i, doc in enumerate(X_test_sent):\n",
    "    wordTokens = text_to_word_sequence(doc)\n",
    "    for j, word in enumerate(wordTokens):\n",
    "        try:\n",
    "            if j<MAX_SEQUENCE and tokenizer.word_index[word]<MAX_NB_WORDS:\n",
    "                test_data[i,j] = tokenizer.word_index[word]\n",
    "        except KeyError as error:\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(667, 20)"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 83, 460,   1, 952,   4,   1, 461, 555,  19, 102,  19, 349,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0])"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = np.arange(data.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "data = data[indices]\n",
    "y = y_train_sent[indices]\n",
    "nb_validation_samples = int(VALIDATION_SPLIT * data.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = data[:-nb_validation_samples]\n",
    "y_train = y[:-nb_validation_samples]\n",
    "x_val = data[-nb_validation_samples:]\n",
    "y_val = y[-nb_validation_samples:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix = np.random.random((len(word_index) + 1, EMBEDDING_DIM))\n",
    "\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "\n",
    "embedding_layer = Embedding(len(word_index)+1,\n",
    "                               EMBEDDING_DIM,\n",
    "                               weights=[embedding_matrix],\n",
    "                               input_length=MAX_SEQUENCE,\n",
    "                               trainable=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__call__() missing 1 required positional argument: 'inputs'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-238-f4876143eb5d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0msentence_input\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mInput\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mMAX_SEQUENCE\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'int32'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;31m# embedded_sequences = embedding_layer(sentence_input)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0membedded_sequences\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0membedding_layer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mMAX_SEQUENCE\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0msentence_gru\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mBidirectional\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mGRU\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m50\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreturn_sequences\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0membedded_sequences\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0msentence_dense\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTimeDistributed\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mDense\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m20\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msentence_gru\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# 10\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: __call__() missing 1 required positional argument: 'inputs'"
     ]
    }
   ],
   "source": [
    "sentence_input = Input(shape=(MAX_SEQUENCE,), dtype='int32')\n",
    "embedded_sequences = embedding_layer(sentence_input)\n",
    "# embedded_sequences = embedding_layer()\n",
    "sentence_gru = Bidirectional(GRU(50, return_sequences=True))(embedded_sequences)\n",
    "sentence_dense = TimeDistributed(Dense(20))(sentence_gru) # 10\n",
    "sentence_att = AttentionWithContext()(sentence_dense)\n",
    "base_model = Model(embedded_sequences, sentence_att)\n",
    "\n",
    "# output = Dense(1, activation='sigmoid')(base_model)\n",
    "\n",
    "# model = base_model(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_15 (InputLayer)        (None, 20)                0         \n",
      "_________________________________________________________________\n",
      "embedding_10 (Embedding)     (None, 20, 100)           453300    \n",
      "_________________________________________________________________\n",
      "bidirectional_13 (Bidirectio (None, 20, 100)           45300     \n",
      "_________________________________________________________________\n",
      "time_distributed_12 (TimeDis (None, 20, 20)            2020      \n",
      "_________________________________________________________________\n",
      "attention_with_context_10 (A (None, 20)                440       \n",
      "_________________________________________________________________\n",
      "dense_22 (Dense)             (None, 1)                 21        \n",
      "=================================================================\n",
      "Total params: 501,081\n",
      "Trainable params: 501,081\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model fitting - Hierarchical attention network\n",
      "Train on 1067 samples, validate on 266 samples\n",
      "Epoch 1/50\n",
      "1067/1067 [==============================] - 4s 3ms/step - loss: 0.7062 - acc: 0.5108 - val_loss: 0.7187 - val_acc: 0.5000\n",
      "Epoch 2/50\n",
      "1067/1067 [==============================] - 1s 1ms/step - loss: 0.6897 - acc: 0.5717 - val_loss: 0.6906 - val_acc: 0.5075\n",
      "Epoch 3/50\n",
      "1067/1067 [==============================] - 1s 1ms/step - loss: 0.6714 - acc: 0.5764 - val_loss: 0.6695 - val_acc: 0.5526\n",
      "Epoch 4/50\n",
      "1067/1067 [==============================] - 1s 1ms/step - loss: 0.6263 - acc: 0.6729 - val_loss: 0.6374 - val_acc: 0.6316\n",
      "Epoch 5/50\n",
      "1067/1067 [==============================] - 1s 1ms/step - loss: 0.4933 - acc: 0.7704 - val_loss: 0.5621 - val_acc: 0.7331\n",
      "Epoch 6/50\n",
      "1067/1067 [==============================] - 1s 1ms/step - loss: 0.3421 - acc: 0.8585 - val_loss: 0.6049 - val_acc: 0.7256\n",
      "Epoch 7/50\n",
      "1067/1067 [==============================] - 1s 1ms/step - loss: 0.2594 - acc: 0.8857 - val_loss: 0.6350 - val_acc: 0.7481\n",
      "Epoch 8/50\n",
      "1067/1067 [==============================] - 1s 1ms/step - loss: 0.1577 - acc: 0.9353 - val_loss: 0.9055 - val_acc: 0.6805\n",
      "Epoch 9/50\n",
      "1067/1067 [==============================] - 1s 1ms/step - loss: 0.1028 - acc: 0.9625 - val_loss: 0.7623 - val_acc: 0.6955\n",
      "Epoch 10/50\n",
      "1067/1067 [==============================] - 1s 1ms/step - loss: 0.0655 - acc: 0.9822 - val_loss: 1.0633 - val_acc: 0.7556\n",
      "Epoch 11/50\n",
      "1067/1067 [==============================] - 1s 1ms/step - loss: 0.0887 - acc: 0.9606 - val_loss: 1.2865 - val_acc: 0.6617\n",
      "Epoch 12/50\n",
      "1067/1067 [==============================] - 1s 1ms/step - loss: 0.0410 - acc: 0.9869 - val_loss: 1.0616 - val_acc: 0.7368\n",
      "Epoch 13/50\n",
      "1067/1067 [==============================] - 1s 1ms/step - loss: 0.0332 - acc: 0.9906 - val_loss: 1.3451 - val_acc: 0.6992\n",
      "Epoch 14/50\n",
      "1067/1067 [==============================] - 1s 1ms/step - loss: 0.0248 - acc: 0.9897 - val_loss: 1.0834 - val_acc: 0.7218\n",
      "Epoch 15/50\n",
      "1067/1067 [==============================] - 1s 1ms/step - loss: 0.0115 - acc: 0.9981 - val_loss: 1.2631 - val_acc: 0.7293\n",
      "Epoch 16/50\n",
      "1067/1067 [==============================] - 1s 1ms/step - loss: 0.0019 - acc: 1.0000 - val_loss: 1.5633 - val_acc: 0.7331\n",
      "Epoch 17/50\n",
      "1067/1067 [==============================] - 1s 1ms/step - loss: 0.0016 - acc: 1.0000 - val_loss: 1.5115 - val_acc: 0.7331\n",
      "Epoch 18/50\n",
      "1067/1067 [==============================] - 1s 1ms/step - loss: 5.3213e-04 - acc: 1.0000 - val_loss: 1.6473 - val_acc: 0.7293\n",
      "Epoch 19/50\n",
      "1067/1067 [==============================] - 1s 1ms/step - loss: 3.9252e-04 - acc: 1.0000 - val_loss: 1.6242 - val_acc: 0.7331\n",
      "Epoch 20/50\n",
      "1067/1067 [==============================] - 1s 1ms/step - loss: 2.9817e-04 - acc: 1.0000 - val_loss: 1.7043 - val_acc: 0.7331\n",
      "Epoch 21/50\n",
      "1067/1067 [==============================] - 1s 1ms/step - loss: 2.2308e-04 - acc: 1.0000 - val_loss: 1.7350 - val_acc: 0.7368\n",
      "Epoch 22/50\n",
      "1067/1067 [==============================] - 1s 1ms/step - loss: 1.8997e-04 - acc: 1.0000 - val_loss: 1.7862 - val_acc: 0.7331\n",
      "Epoch 23/50\n",
      "1067/1067 [==============================] - 1s 1ms/step - loss: 1.6313e-04 - acc: 1.0000 - val_loss: 1.8246 - val_acc: 0.7331\n",
      "Epoch 24/50\n",
      "1067/1067 [==============================] - 1s 1ms/step - loss: 1.4298e-04 - acc: 1.0000 - val_loss: 1.8265 - val_acc: 0.7293\n",
      "Epoch 25/50\n",
      "1067/1067 [==============================] - 1s 1ms/step - loss: 1.1994e-04 - acc: 1.0000 - val_loss: 1.8680 - val_acc: 0.7331\n",
      "Epoch 26/50\n",
      "1067/1067 [==============================] - 1s 1ms/step - loss: 1.0901e-04 - acc: 1.0000 - val_loss: 1.8889 - val_acc: 0.7331\n",
      "Epoch 27/50\n",
      "1067/1067 [==============================] - 1s 1ms/step - loss: 9.7736e-05 - acc: 1.0000 - val_loss: 1.9076 - val_acc: 0.7368\n",
      "Epoch 28/50\n",
      "1067/1067 [==============================] - 1s 1ms/step - loss: 8.7562e-05 - acc: 1.0000 - val_loss: 1.9297 - val_acc: 0.7368\n",
      "Epoch 29/50\n",
      "1067/1067 [==============================] - 1s 1ms/step - loss: 7.9254e-05 - acc: 1.0000 - val_loss: 1.9550 - val_acc: 0.7368\n",
      "Epoch 30/50\n",
      "1067/1067 [==============================] - 1s 1ms/step - loss: 7.3477e-05 - acc: 1.0000 - val_loss: 1.9757 - val_acc: 0.7368\n",
      "Epoch 31/50\n",
      "1067/1067 [==============================] - 1s 1ms/step - loss: 6.6834e-05 - acc: 1.0000 - val_loss: 1.9926 - val_acc: 0.7368\n",
      "Epoch 32/50\n",
      "1067/1067 [==============================] - 1s 1ms/step - loss: 6.1460e-05 - acc: 1.0000 - val_loss: 2.0119 - val_acc: 0.7368\n",
      "Epoch 33/50\n",
      "1067/1067 [==============================] - 1s 1ms/step - loss: 5.6424e-05 - acc: 1.0000 - val_loss: 2.0224 - val_acc: 0.7368\n",
      "Epoch 34/50\n",
      "1067/1067 [==============================] - 1s 1ms/step - loss: 5.2719e-05 - acc: 1.0000 - val_loss: 2.0406 - val_acc: 0.7368\n",
      "Epoch 35/50\n",
      "1067/1067 [==============================] - 1s 1ms/step - loss: 4.8720e-05 - acc: 1.0000 - val_loss: 2.0600 - val_acc: 0.7368\n",
      "Epoch 36/50\n",
      "1067/1067 [==============================] - 1s 1ms/step - loss: 4.5359e-05 - acc: 1.0000 - val_loss: 2.0711 - val_acc: 0.7368\n",
      "Epoch 37/50\n",
      "1067/1067 [==============================] - 1s 1ms/step - loss: 4.2361e-05 - acc: 1.0000 - val_loss: 2.0922 - val_acc: 0.7368\n",
      "Epoch 38/50\n",
      "1067/1067 [==============================] - 1s 1ms/step - loss: 3.9933e-05 - acc: 1.0000 - val_loss: 2.0991 - val_acc: 0.7368\n",
      "Epoch 39/50\n",
      "1067/1067 [==============================] - 1s 1ms/step - loss: 3.7221e-05 - acc: 1.0000 - val_loss: 2.1195 - val_acc: 0.7368\n",
      "Epoch 40/50\n",
      "1067/1067 [==============================] - 1s 1ms/step - loss: 3.5078e-05 - acc: 1.0000 - val_loss: 2.1296 - val_acc: 0.7368\n",
      "Epoch 41/50\n",
      "1067/1067 [==============================] - 1s 1ms/step - loss: 3.3178e-05 - acc: 1.0000 - val_loss: 2.1434 - val_acc: 0.7368\n",
      "Epoch 42/50\n",
      "1067/1067 [==============================] - 1s 1ms/step - loss: 3.1774e-05 - acc: 1.0000 - val_loss: 2.1581 - val_acc: 0.7368\n",
      "Epoch 43/50\n",
      "1067/1067 [==============================] - 1s 1ms/step - loss: 2.9347e-05 - acc: 1.0000 - val_loss: 2.1737 - val_acc: 0.7368\n",
      "Epoch 44/50\n",
      "1067/1067 [==============================] - 1s 1ms/step - loss: 2.7775e-05 - acc: 1.0000 - val_loss: 2.1837 - val_acc: 0.7368\n",
      "Epoch 45/50\n",
      "1067/1067 [==============================] - 1s 1ms/step - loss: 2.6636e-05 - acc: 1.0000 - val_loss: 2.2001 - val_acc: 0.7368\n",
      "Epoch 46/50\n",
      "1067/1067 [==============================] - 1s 1ms/step - loss: 2.4916e-05 - acc: 1.0000 - val_loss: 2.2110 - val_acc: 0.7368\n",
      "Epoch 47/50\n",
      "1067/1067 [==============================] - 1s 1ms/step - loss: 2.3759e-05 - acc: 1.0000 - val_loss: 2.2227 - val_acc: 0.7368\n",
      "Epoch 48/50\n",
      "1067/1067 [==============================] - 1s 1ms/step - loss: 2.2541e-05 - acc: 1.0000 - val_loss: 2.2334 - val_acc: 0.7368\n",
      "Epoch 49/50\n",
      "1067/1067 [==============================] - 1s 1ms/step - loss: 2.1516e-05 - acc: 1.0000 - val_loss: 2.2452 - val_acc: 0.7406\n",
      "Epoch 50/50\n",
      "1067/1067 [==============================] - 1s 1ms/step - loss: 2.0538e-05 - acc: 1.0000 - val_loss: 2.2540 - val_acc: 0.7368\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1c47d0b96d8>"
      ]
     },
     "execution_count": 220,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('model fitting - Hierarchical attention network')\n",
    "model.fit(x_train, y_train, validation_data=(x_val, y_val),\n",
    "           epochs=50, batch_size=32, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_input = Input(shape=(MAX_SEQUENCE,), dtype='int32')\n",
    "embedded_sequences = embedding_layer(sentence_input)\n",
    "sentence_gru = Bidirectional(GRU(50, return_sequences=False))(embedded_sequences)\n",
    "# sentence_dense = TimeDistributed(Dense(10))(sentence_gru)\n",
    "# sentence_att = AttentionWithContext()(sentence_dense)\n",
    "sentence_dense = Dense(10)(sentence_gru)\n",
    "\n",
    "output = Dense(1, activation='sigmoid')(sentence_dense)\n",
    "baseline = Model(sentence_input, output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline.fit(x_train, y_train, validation_data=(x_val, y_val),\n",
    "           epochs=10, batch_size=32, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_3rd_layer_output = K.function([model.layers[0].input],\n",
    "                                  [model.layers[4].output])\n",
    "layer_output = get_3rd_layer_output([x_train])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-2.4435644 , -3.2460752 ,  1.5075619 , -2.5178852 ,  1.3407952 ,\n",
       "        0.50468725,  2.0410352 , -0.5291209 ,  1.5506485 ,  0.73354095,\n",
       "        1.3296204 ,  1.3797984 ,  1.8990283 , -2.478027  ,  0.41687998,\n",
       "        1.8729966 , -1.4615527 , -2.8485737 ,  1.638824  ,  2.6011367 ],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 222,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer_output[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-2.4435644 , -3.2460752 ,  1.5075619 , -2.5178852 ,  1.3407952 ,\n",
       "        0.50468725,  2.0410352 , -0.5291209 ,  1.5506485 ,  0.73354095,\n",
       "        1.3296204 ,  1.3797984 ,  1.8990283 , -2.478027  ,  0.41687998,\n",
       "        1.8729966 , -1.4615527 , -2.8485737 ,  1.638824  ,  2.6011367 ],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer_output[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1067, 20)"
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer_output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sentence_attention = get_3rd_layer_output([data])[0]\n",
    "test_sentence_attention = get_3rd_layer_output([test_data])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_attention = np.vstack([train_sentence_attention,test_sentence_attention])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2000, 20)"
      ]
     },
     "execution_count": 227,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_attention.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import NearestNeighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "nbrs = NearestNeighbors(n_neighbors=7, algorithm='ball_tree').fit(all_attention)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "distances, indices = nbrs.kneighbors(all_attention)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict = model.predict(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ she becomes the centre of the films universe as well as our ------\n",
      "0.0\n",
      "[0.9999862]\n",
      "\n",
      "1 for free\n",
      "\t label: 0.0\t0.3960403817586489\n",
      "2 especially for those of you who enjoy all those japanese chambara samurai and ninja film you definitely have to see blood\n",
      "\t label: 1.0\t0.4030929276591905\n",
      "3 there are moment when she almost teeter but she consistently exudes charm\n",
      "\t label: 0.0\t0.44034667795403676\n",
      "4 please do not mantion marlon brando in the same breath of this mansee taxi driver for confirmation of this point\n",
      "\t label: 0.0\t0.4765088485779896\n",
      "5 i am sorry it did not materialize into a series\n",
      "\t label: 0.0\t0.5292432473130144\n",
      "6 joe haggerty gives a spirited and very funny performance as ebenezer jackson\n",
      "\t label: 1.0\t0.5378928816138699\n",
      "\n",
      "------ anyone who loves the rheostatic music is going to enjoy this film ------\n",
      "1.0\n",
      "[3.2664113e-07]\n",
      "\n",
      "1 jack nicholson barely fits into jack torrences character\n",
      "\t label: 0.0\t0.3341763926070872\n",
      "2 he then goes on the prowl looking for the perfect body to make her whole again\n",
      "\t label: 0.0\t0.36841478731820204\n",
      "3 the supporting cast most of the folk from the first film as well as jeff goldblum saffron burrow and a muchwelcomed return from 90s indiedarling elina lowensohn is excellent and the film has lot of surprise\n",
      "\t label: 1.0\t0.3795026946271961\n",
      "4 moreoveryou think you are going to watch a martial art with about a girl engulfed in vengence for her parent death but surprise\n",
      "\t label: 0.0\t0.3946585709632387\n",
      "5 you have to see it to believe it\n",
      "\t label: 0.0\t0.3982795978653189\n",
      "6 this was so lame that i turned the dvd offmaybe halfway through\n",
      "\t label: 1.0\t0.3986715656268898\n",
      "\n",
      "------ the song that were in common with the musical are better done in the movie the new one are quite good one and the whole movie just delivers more than the musical in my opinion especially compared to a musical which has few decor ------\n",
      "1.0\n",
      "[2.1362956e-07]\n",
      "\n",
      "1 i was on france around march 05 and i love to go to this film festival\n",
      "\t label: 0.0\t0.28428507307272843\n",
      "2 no twist or turn will make this a more interesting train wreck or any different from any of the other\n",
      "\t label: 1.0\t0.29319573936981175\n",
      "3 at a panel discussion that i attended after viewing this film the filmmaker stated that one should look at this not as a movie but a provoker of thought\n",
      "\t label: 0.0\t0.29988681948939927\n",
      "4 joan fontaine is a damsel in distress in this 1937 musical starring fred astaire george burn and gracie allen\n",
      "\t label: 0.0\t0.302068035872983\n",
      "5 suppose you have been on a deserted island the last ten year\n",
      "\t label: 0.0\t0.3331220995454071\n",
      "6 i felt really sorry for toby will thorp when he came out of the possession for the 2nd time because he was so scared\n",
      "\t label: 0.0\t0.351581935784559\n",
      "\n",
      "------ disappointing and undeniably dull truecrime movie that has poorly cast character actor jeremy renner languidly mumbling his way through the title role of jeffrey dahmer who was easily one of the last centurys most recognizable degeneratesserial killer ------\n",
      "0.0\n",
      "[0.9999962]\n",
      "\n",
      "1 it is a perfect movie with wonderful dancing\n",
      "\t label: 1.0\t0.38973404117418353\n",
      "2 adam sandler is hired to work on a giant cruise ship with some m universe model and five other person\n",
      "\t label: 0.0\t0.41510391996888796\n",
      "3 when i was six yo i learned about a series called los campeone and even if i was just a kid i did everything i need to convince my pare not to let me watch the champion and the avenger once every week\n",
      "\t label: 0.0\t0.4202278273667421\n",
      "4 this film is 75 minute long and feel like three and a half hour\n",
      "\t label: 1.0\t0.434031498395235\n",
      "5 an expedition is sent out from earth to the fourth planet of altair a great mainsequence star in constellation aquilae to find out what happened to a colony of settler which landed twenty year before and had not been heard from since\n",
      "\t label: 0.0\t0.47289523167990666\n",
      "6 you should watch yugoslaviadeath of a nation made by discovery channel and bbc\n",
      "\t label: 0.0\t0.4850798809856201\n",
      "\n",
      "------ i would not recommend to anyone that they waste the time it takes to watch it ------\n",
      "1.0\n",
      "[1.3795557e-06]\n",
      "\n",
      "1 what is very french about this film is the time taken to establish the two leading character\n",
      "\t label: 0.0\t0.38789070942680187\n",
      "2 but do not get me wrong dode kaden may not be kurosawas best but coming from the greatest director of all time it is much better than 99 of todays film\n",
      "\t label: 1.0\t0.39619293516881227\n",
      "3 maybe i am missing something i just can not believe a movie like this can beat a classic like hddcs\n",
      "\t label: 1.0\t0.40454154491557603\n",
      "4 dear god i do not know where to start why this movie sucked too much\n",
      "\t label: 1.0\t0.40832088039878583\n",
      "5 yeah it is worth watching if you are real bored and you want to reaffirm the fact that anyone can make a movie or at least can try to\n",
      "\t label: 1.0\t0.41201464795611326\n",
      "6 movie theater management would not allow this but they did agree to let us see another film\n",
      "\t label: 0.0\t0.4194728307244135\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    print(\"------ \" + X_train_sent[i] + \" ------\")\n",
    "    print(y_train_sent[i])\n",
    "    print(predict[i])\n",
    "    print()\n",
    "    \n",
    "    for j in range(7):\n",
    "        if j == 0:\n",
    "            continue\n",
    "        if indices[i, j] < 1333 :\n",
    "            print(str(j) + \" \" + X_train_sent[indices[i, j]])\n",
    "            print(\"\\t label: \" + str(y_train_sent[indices[i, j]]) + \"\\t\" + str(distances[i,j]))\n",
    "        else:\n",
    "            print(str(j) + \" \" + X_test_sent[indices[i,j]-1333])\n",
    "            print(\"\\t label: \" + str(y_test_sent[indices[i,j]-1333]) + \"\\t\" + str(distances[i,j]))\n",
    "    \n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
