{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "from textblob import TextBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open_pickle\n",
    "\n",
    "def open_pickle(path):\n",
    "    with open(path, 'rb') as f:\n",
    "        X = pickle.load(f)\n",
    "    return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load dataset of [relevant,-relevant]\n",
    "\n",
    "X_train_sentence = open_pickle('../data/imdb-sentence/imdb_sentence_xtrain.pickle')\n",
    "X_test_sentence = open_pickle('../data/imdb-sentence/imdb_sentence_xtest.pickle')\n",
    "y_train_sentence = open_pickle('../data/imdb-sentence/imdb_sentence_ytrain.pickle')\n",
    "y_test_sentence = open_pickle('../data/imdb-sentence/imdb_sentence_ytest.pickle')\n",
    "\n",
    "#Load dataset of [+/-]\n",
    "\n",
    "X_train_original = open_pickle('../data/imdb/imdb_original_preprocessed_xtrain.pickle')\n",
    "X_test_original = open_pickle('../data/imdb/imdb_original_preprocessed_xtest.pickle')\n",
    "y_train_original = open_pickle('../data/imdb/imdb_original_preprocessed_ytrain.pickle')\n",
    "y_test_original = open_pickle('../data/imdb/imdb_original_preprocessed_ytest.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26266\n"
     ]
    }
   ],
   "source": [
    "# Baseline\n",
    "\n",
    "token = r\"(?u)\\b[\\w\\'/]+\\b\"\n",
    "cv = CountVectorizer(lowercase=True, min_df=5, binary=True, token_pattern=token)\n",
    "\n",
    "X_tr_baseline = cv.fit_transform(X_train_original)\n",
    "X_te_baseline = cv.transform(X_test_original)\n",
    "\n",
    "print(len(cv.get_feature_names()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25000, 26266)\n"
     ]
    }
   ],
   "source": [
    "print(X_tr_baseline.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.90968\n",
      "0.8794\n"
     ]
    }
   ],
   "source": [
    "clf = LogisticRegression(random_state=42, C=0.01)\n",
    "\n",
    "clf.fit(X_tr_baseline, y_train_original)\n",
    "\n",
    "\n",
    "print(clf.score(X_tr_baseline, y_train_original))\n",
    "print(clf.score(X_te_baseline, y_test_original))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "546\n"
     ]
    }
   ],
   "source": [
    "# Count Vectorizer on rel,unrel dataset\n",
    "# Question : Why rel/unrel? Because it trained as the first step? \n",
    "# Any advantages on more vocabulary?\n",
    "\n",
    "token = r\"(?u)\\b[\\w\\'/]+\\b\"\n",
    "tf_vectorizer = CountVectorizer(lowercase=True, max_df=1.0, min_df=5, binary=True, token_pattern=token)\n",
    "tf_vectorizer.set_params(ngram_range=(1,1))\n",
    "\n",
    "X_train_sentence_bow = tf_vectorizer.fit_transform(X_train_sentence)\n",
    "X_test_sentence_bow = tf_vectorizer.transform(X_test_sentence)\n",
    "\n",
    "X_train_original_bow = tf_vectorizer.transform(X_train_original)\n",
    "X_test_original_bow = tf_vectorizer.transform(X_test_original)\n",
    "\n",
    "\n",
    "words = tf_vectorizer.get_feature_names()\n",
    "print(len(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Again baseline\n",
    "\n",
    "clf = LogisticRegression(random_state=42, C=0.01)\n",
    "\n",
    "clf.fit(X_train_original_bow, y_train_original)\n",
    "\n",
    "print(clf.score(X_train_original_bow, y_train_original))\n",
    "print(clf.score(X_test_original_bow, y_test_original))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y_train_sentence.shape)\n",
    "print(y_test_sentence.shape)\n",
    "print(y_train_original.shape)\n",
    "print(y_test_original.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train to [rel,unrel] classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Okay... Using the function makes me more overwhelmed. Let's do it manually.\n",
    "\n",
    "\n",
    "\n",
    "random_state = 42\n",
    "C = 0.01\n",
    "\n",
    "clf_A = LogisticRegression(random_state=random_state, C=C)\n",
    "clf_A.fit(X_train_sentence_bow, y_train_sentence)\n",
    "\n",
    "y_predict = clf_A.predict(X_test_sentence_bow)\n",
    "\n",
    "print('--------------')\n",
    "print('C=%.2f' %(C))\n",
    "print('--------------')\n",
    "print('Accuracy')\n",
    "print('Train:\\t%.5f ' %(clf_A.score(X_train_sentence_bow, y_train_sentence)))\n",
    "print('Test:\\t%.5f ' %(clf_A.score(X_test_sentence_bow, y_test_sentence)))\n",
    "    \n",
    "print(classification_report(y_test_sentence,y_predict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train [+,-] classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using whole corpus\n",
    "\n",
    "clf_1 = LogisticRegression(random_state=random_state, C=C)\n",
    "clf_1.fit(X_train_original_bow, y_train_original)\n",
    "\n",
    "# using the [+/-] sentence\n",
    "# clf_2 = LogisticRegression(random_state=random_state, C=C)\n",
    "# clf_2.fit(X_train_sentence_bow, y_train_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test on a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TextBlob(X_train_original[1]).raw_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_original[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TextBlob(X_train_original[5]).raw_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set = tf_vectorizer.transform(TextBlob(X_train_original[5]).raw_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,j in enumerate(clf_A.predict_proba(test_set)[:,1]):\n",
    "    if j>0.5:\n",
    "        print(i,TextBlob(X_train_original[5]).raw_sentences[i],\"\\t{:.3f}\".format(j))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(np.array(x).flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.where(clf_A.predict_proba(test_set)[:,1]>0.5)\n",
    "\n",
    "test = test_set[x]\n",
    "\n",
    "for i,j in enumerate(clf_1.predict_proba(test)):\n",
    "    print(i,TextBlob(X_train_original[5]).raw_sentences[np.array(x).flatten()[i]],j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build an accuracy function excluding those -1\n",
    "\n",
    "\n",
    "y_pred = []\n",
    "threshold=None\n",
    "highest_confidence_related=True\n",
    "\n",
    "for corpus in X_train_original[0:1]:\n",
    "    '''\n",
    "    Breakdown the corpus into sentence and transform into bag-of-words\n",
    "    '''\n",
    "    sentence_set = tf_vectorizer.transform(TextBlob(corpus).raw_sentences)\n",
    "    \n",
    "    '''\n",
    "    Related classifier given threshold. \n",
    "    threshold, if None, it only predict the label. If float number given, assign the threshold to the sentence with\n",
    "    probability over threshold\n",
    "    '''\n",
    "    if threshold==None:\n",
    "        y_ind_proba = clf_A.predict_proba(sentence_set)\n",
    "        y_ind = clf_A.predict(sentence_set)\n",
    "    else:\n",
    "        y_ind_proba = clf_A.predict_proba(sentence_set)\n",
    "        y_ind = y_ind_proba[:,1]>threshold\n",
    "        \n",
    "    '''\n",
    "    +/- classifier given the sentences from the previous classifier\n",
    "    if 0, assign the label as -1 (which means it does not have any related sentence)\n",
    "    else, classify the sentence into +/- label according to the given sentence. \n",
    "    '''\n",
    "    if np.sum(y_ind) == 0:\n",
    "        y_pred.append(-1)\n",
    "        continue\n",
    "    else:\n",
    "        if highest_confidence_related:\n",
    "            indices = np.array(np.argmax(y_ind_proba[:,1])).flatten()\n",
    "            y = clf_1.predict(sentence_set[indices, :])\n",
    "            y_pred.append(y)\n",
    "        else:\n",
    "            indices = np.array(np.where(y_ind[:,1] == 1)).flatten()\n",
    "            y_np_proba = clf_1.predict_proba(sentence_set[indices, :])\n",
    "            y_neg_proba = np.max(y_np_proba[:,0])\n",
    "            y_pos_proba = np.max(y_np_proba[:,1])\n",
    "            mn, mp = np.max(y_np_proba, axis=1)\n",
    "\n",
    "            if y_pos_proba > y_neg_proba:\n",
    "                y_pred.append(1)\n",
    "            else:\n",
    "                y_pred.append(0)\n",
    "\n",
    "     \n",
    "    \n",
    "y_pred = np.array(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_original[1:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
