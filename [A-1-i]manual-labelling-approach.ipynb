{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "from textblob import TextBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open_pickle\n",
    "\n",
    "def open_pickle(path):\n",
    "    with open(path, 'rb') as f:\n",
    "        X = pickle.load(f)\n",
    "    return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load dataset of sentence [relevant,-relevant]\n",
    "\n",
    "X_train_sentence = open_pickle('../data/imdb-sentence/imdb_sentence_xtrain.pickle')\n",
    "X_test_sentence = open_pickle('../data/imdb-sentence/imdb_sentence_xtest.pickle')\n",
    "y_train_sentence = open_pickle('../data/imdb-sentence/imdb_sentence_ytrain.pickle')\n",
    "y_test_sentence = open_pickle('../data/imdb-sentence/imdb_sentence_ytest.pickle')\n",
    "\n",
    "#Load dataset of [whole corpus]\n",
    "\n",
    "X_train_original = open_pickle('../data/imdb/imdb_original_preprocessed_xtrain.pickle')\n",
    "X_test_original = open_pickle('../data/imdb/imdb_original_preprocessed_xtest.pickle')\n",
    "y_train_original = open_pickle('../data/imdb/imdb_original_preprocessed_ytrain.pickle')\n",
    "y_test_original = open_pickle('../data/imdb/imdb_original_preprocessed_ytest.pickle')\n",
    "\n",
    "#Load dataset of sentence [+/-]\n",
    "\n",
    "X_train_np_sentence = open_pickle('../data/imdb-sentence/imdb_sentence_np_xtrain.pickle')\n",
    "X_test_np_sentence = open_pickle('../data/imdb-sentence/imdb_sentence_np_xtest.pickle')\n",
    "y_train_np_sentence = open_pickle('../data/imdb-sentence/imdb_sentence_np_ytrain.pickle')\n",
    "y_test_np_sentence = open_pickle('../data/imdb-sentence/imdb_sentence_np_ytest.pickle')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "546\n"
     ]
    }
   ],
   "source": [
    "# Count Vectorizer on rel,unrel dataset\n",
    "# Question : Why rel/unrel? Because it trained as the first step? \n",
    "# Any advantages on more vocabulary?\n",
    "\n",
    "token = r\"(?u)\\b[\\w\\'/]+\\b\"\n",
    "tf_vectorizer = CountVectorizer(lowercase=True, max_df=1.0, min_df=5, binary=True, token_pattern=token)\n",
    "tf_vectorizer.set_params(ngram_range=(1,1))\n",
    "\n",
    "# rel/unrel sentence\n",
    "X_train_sentence_bow = tf_vectorizer.fit_transform(X_train_sentence)\n",
    "X_test_sentence_bow = tf_vectorizer.transform(X_test_sentence)\n",
    "\n",
    "# whole imdb corpus\n",
    "X_train_original_bow = tf_vectorizer.transform(X_train_original)\n",
    "X_test_original_bow = tf_vectorizer.transform(X_test_original)\n",
    "\n",
    "# neg/pos sentence\n",
    "X_train_np_bow = tf_vectorizer.transform(X_train_np_sentence)\n",
    "X_test_np_bow = tf_vectorizer.transform(X_test_np_sentence) \n",
    "\n",
    "words = tf_vectorizer.get_feature_names()\n",
    "print(len(words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train A [rel,unrel] classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------\n",
      "C=0.10\n",
      "--------------\n",
      "Accuracy\n",
      "Train:\t0.82671 \n",
      "Test:\t0.75712 \n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.80      0.73      0.77       363\n",
      "        1.0       0.71      0.79      0.75       304\n",
      "\n",
      "avg / total       0.76      0.76      0.76       667\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Okay... Using the function makes me more overwhelmed. Let's do it manually.\n",
    "\n",
    "\n",
    "random_state = 42\n",
    "C = 0.1\n",
    "\n",
    "clf_A = LogisticRegression(random_state=random_state, C=C)\n",
    "clf_A.fit(X_train_sentence_bow, y_train_sentence)\n",
    "\n",
    "y_predict = clf_A.predict(X_test_sentence_bow)\n",
    "\n",
    "print('--------------')\n",
    "print('C=%.2f' %(C))\n",
    "print('--------------')\n",
    "print('Accuracy')\n",
    "print('Train:\\t%.5f ' %(clf_A.score(X_train_sentence_bow, y_train_sentence)))\n",
    "print('Test:\\t%.5f ' %(clf_A.score(X_test_sentence_bow, y_test_sentence)))\n",
    "    \n",
    "print(classification_report(y_test_sentence,y_predict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train 1 [+,-] classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=42, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# using whole corpus\n",
    "clf_1_i = LogisticRegression(random_state=random_state, C=C)\n",
    "clf_1_i.fit(X_train_original_bow, y_train_original)\n",
    "\n",
    "# using the [+/-] sentence\n",
    "\n",
    "clf_1_j = LogisticRegression(random_state=random_state, C=C)\n",
    "clf_1_j.fit(X_train_np_bow, y_train_np_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------\n",
      "C=0.10\n",
      "--------------\n",
      "Accuracy\n",
      "Train:\t0.84748 \n",
      "Test:\t0.84212 \n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.85      0.83      0.84     12500\n",
      "          1       0.83      0.85      0.84     12500\n",
      "\n",
      "avg / total       0.84      0.84      0.84     25000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Test baseline\n",
    "\n",
    "y_predict = clf_1_i.predict(X_test_original_bow)\n",
    "\n",
    "print('--------------')\n",
    "print('C=%.2f' %(C))\n",
    "print('--------------')\n",
    "print('Accuracy')\n",
    "print('Train:\\t%.5f ' %(clf_1_i.score(X_train_original_bow, y_train_original)))\n",
    "print('Test:\\t%.5f ' %(clf_1_i.score(X_test_original_bow, y_test_original)))\n",
    "    \n",
    "print(classification_report(y_test_original,y_predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------\n",
      "C=0.10\n",
      "--------------\n",
      "Accuracy\n",
      "Train:\t0.84084 \n",
      "Test:\t0.72156 \n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.73      0.68      0.70       163\n",
      "        1.0       0.71      0.76      0.74       171\n",
      "\n",
      "avg / total       0.72      0.72      0.72       334\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Test baseline\n",
    "\n",
    "y_predict = clf_1_j.predict(X_test_np_bow)\n",
    "\n",
    "print('--------------')\n",
    "print('C=%.2f' %(C))\n",
    "print('--------------')\n",
    "print('Accuracy')\n",
    "print('Train:\\t%.5f ' %(clf_1_j.score(X_train_np_bow, y_train_np_sentence)))\n",
    "print('Test:\\t%.5f ' %(clf_1_j.score(X_test_np_bow, y_test_np_sentence)))\n",
    "    \n",
    "print(classification_report(y_test_np_sentence,y_predict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now the real deal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build an accuracy function excluding those -1\n",
    "\n",
    "def nested_classifier(X_test_original, threshold):\n",
    "    y_pred_i = []\n",
    "    y_pred_j = []\n",
    "    \n",
    "    for ind, corpus in enumerate(X_test_original):\n",
    "        '''\n",
    "        Breakdown the corpus into sentence and transform into bag-of-words\n",
    "        '''\n",
    "        sentence_set = tf_vectorizer.transform(TextBlob(corpus).raw_sentences)\n",
    "\n",
    "        '''\n",
    "        related sentence classifier\n",
    "        '''\n",
    "        y_A_proba = clf_A.predict_proba(sentence_set)\n",
    "        mu, mr = np.argmax(y_A_proba, axis=0)\n",
    "\n",
    "        '''\n",
    "        +/- classifier\n",
    "        '''\n",
    "        if y_A_proba[mr,1] > threshold:\n",
    "            y_i_proba = clf_1_i.predict_proba(sentence_set[mr])\n",
    "            y_pred_i.append(np.argmax(y_i_proba))\n",
    "\n",
    "            y_j_proba = clf_1_j.predict_proba(sentence_set[mr])\n",
    "            y_pred_j.append(np.argmax(y_j_proba))\n",
    "        else:\n",
    "            y_pred_i.append(-1)\n",
    "            y_pred_j.append(-1)\n",
    "            continue\n",
    "    return np.array(y_pred_i), np.array(y_pred_j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rejection_rate(y):\n",
    "    return np.sum(y==-1)/len(y)\n",
    "\n",
    "def accuracy(y, y_pred):\n",
    "    return np.sum(y_pred==y)/(np.sum(y_pred==1) + np.sum(y_pred==0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t \t ~rel \t rr \t acc_i \t acc_j\n",
      "0.50 \t 525 \t 0.02100 \t 0.71632 \t 0.64547\n",
      "0.55 \t 897 \t 0.03588 \t 0.71701 \t 0.64556\n",
      "0.60 \t 1532 \t 0.06128 \t 0.71834 \t 0.64594\n",
      "0.65 \t 2501 \t 0.10004 \t 0.71994 \t 0.64732\n",
      "0.70 \t 4056 \t 0.16224 \t 0.72522 \t 0.65011\n",
      "0.75 \t 6460 \t 0.25840 \t 0.73069 \t 0.65372\n",
      "0.80 \t 9944 \t 0.39776 \t 0.73698 \t 0.66020\n",
      "0.85 \t 14382 \t 0.57528 \t 0.75353 \t 0.67065\n",
      "0.90 \t 19515 \t 0.78060 \t 0.77302 \t 0.68642\n",
      "0.95 \t 23685 \t 0.94740 \t 0.81445 \t 0.71255\n",
      "0.96 \t 24172 \t 0.96688 \t 0.82488 \t 0.72947\n",
      "0.97 \t 24582 \t 0.98328 \t 0.82536 \t 0.71770\n",
      "0.98 \t 24835 \t 0.99340 \t 0.82424 \t 0.73333\n",
      "0.99 \t 24964 \t 0.99856 \t 0.83333 \t 0.83333\n"
     ]
    }
   ],
   "source": [
    "threshold = np.arange(0.5, 1, 0.05)\n",
    "threshold = np.append(threshold, [0.96, 0.97, 0.98, 0.99])\n",
    "\n",
    "print('t \\t ~rel \\t rr \\t acc_i \\t acc_j')\n",
    "for t in threshold : \n",
    "    y_pred_i, y_pred_j = nested_classifier(X_test_original, t)\n",
    "    print('%.2f \\t %d \\t %.5f \\t %.5f \\t %.5f' %(t,\n",
    "                                                 np.sum(y_pred_i==-1),\n",
    "                                                 rejection_rate(y_pred_i), \n",
    "                                                 accuracy(y_test_original, y_pred_i), \n",
    "                                                 accuracy(y_test_original,y_pred_j)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1\n"
     ]
    }
   ],
   "source": [
    "# np.savetxt('cos_sim.csv', np.around(cos_sim,2), delimiter=',')\n",
    "print(C)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's take a look on the words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
