{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "from textblob import TextBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open_pickle\n",
    "\n",
    "def open_pickle(path):\n",
    "    with open(path, 'rb') as f:\n",
    "        X = pickle.load(f)\n",
    "    return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load dataset of sentence [relevant,-relevant]\n",
    "\n",
    "X_train_sentence = open_pickle('../data/imdb-sentence/imdb_sentence_xtrain.pickle')\n",
    "X_test_sentence = open_pickle('../data/imdb-sentence/imdb_sentence_xtest.pickle')\n",
    "y_train_sentence = open_pickle('../data/imdb-sentence/imdb_sentence_ytrain.pickle')\n",
    "y_test_sentence = open_pickle('../data/imdb-sentence/imdb_sentence_ytest.pickle')\n",
    "\n",
    "#Load dataset of [whole corpus]\n",
    "\n",
    "X_train_original = open_pickle('../data/imdb/imdb_original_preprocessed_xtrain.pickle')\n",
    "X_test_original = open_pickle('../data/imdb/imdb_original_preprocessed_xtest.pickle')\n",
    "y_train_original = open_pickle('../data/imdb/imdb_original_preprocessed_ytrain.pickle')\n",
    "y_test_original = open_pickle('../data/imdb/imdb_original_preprocessed_ytest.pickle')\n",
    "\n",
    "#Load dataset of sentence [+/-]\n",
    "\n",
    "X_train_np_sentence = open_pickle('../data/imdb-sentence/imdb_sentence_np_xtrain.pickle')\n",
    "X_test_np_sentence = open_pickle('../data/imdb-sentence/imdb_sentence_np_xtest.pickle')\n",
    "y_train_np_sentence = open_pickle('../data/imdb-sentence/imdb_sentence_np_ytrain.pickle')\n",
    "y_test_np_sentence = open_pickle('../data/imdb-sentence/imdb_sentence_np_ytest.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26266\n"
     ]
    }
   ],
   "source": [
    "# Baseline\n",
    "\n",
    "token = r\"(?u)\\b[\\w\\'/]+\\b\"\n",
    "cv = CountVectorizer(lowercase=True, min_df=5, binary=True, token_pattern=token)\n",
    "\n",
    "X_tr_baseline = cv.fit_transform(X_train_original)\n",
    "X_te_baseline = cv.transform(X_test_original)\n",
    "\n",
    "print(len(cv.get_feature_names()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25000, 26266)\n"
     ]
    }
   ],
   "source": [
    "print(X_tr_baseline.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.90968\n",
      "0.8794\n"
     ]
    }
   ],
   "source": [
    "clf = LogisticRegression(random_state=42, C=0.01)\n",
    "\n",
    "clf.fit(X_tr_baseline, y_train_original)\n",
    "\n",
    "\n",
    "print(clf.score(X_tr_baseline, y_train_original))\n",
    "print(clf.score(X_te_baseline, y_test_original))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "546\n"
     ]
    }
   ],
   "source": [
    "# Count Vectorizer on rel,unrel dataset\n",
    "# Question : Why rel/unrel? Because it trained as the first step? \n",
    "# Any advantages on more vocabulary?\n",
    "\n",
    "token = r\"(?u)\\b[\\w\\'/]+\\b\"\n",
    "tf_vectorizer = CountVectorizer(lowercase=True, max_df=1.0, min_df=5, binary=True, token_pattern=token)\n",
    "tf_vectorizer.set_params(ngram_range=(1,1))\n",
    "\n",
    "# rel/unrel sentence\n",
    "X_train_sentence_bow = tf_vectorizer.fit_transform(X_train_sentence)\n",
    "X_test_sentence_bow = tf_vectorizer.transform(X_test_sentence)\n",
    "\n",
    "# whole imdb corpus\n",
    "X_train_original_bow = tf_vectorizer.transform(X_train_original)\n",
    "X_test_original_bow = tf_vectorizer.transform(X_test_original)\n",
    "\n",
    "# neg/pos sentence\n",
    "X_train_np_bow = tf_vectorizer.transform(X_train_np_sentence)\n",
    "X_test_np_bow = tf_vectorizer.transform(X_test_np_sentence) \n",
    "\n",
    "words = tf_vectorizer.get_feature_names()\n",
    "print(len(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8416\n",
      "0.83664\n"
     ]
    }
   ],
   "source": [
    "# Again baseline\n",
    "\n",
    "clf = LogisticRegression(random_state=42, C=0.01)\n",
    "\n",
    "clf.fit(X_train_original_bow, y_train_original)\n",
    "\n",
    "print(clf.score(X_train_original_bow, y_train_original))\n",
    "print(clf.score(X_test_original_bow, y_test_original))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corpus\n",
      "(25000,)\n",
      "(25000,)\n",
      "rel/unrel\n",
      "(1333,)\n",
      "(667,)\n",
      "np sentence\n",
      "(666,)\n",
      "(334,)\n"
     ]
    }
   ],
   "source": [
    "print('corpus')\n",
    "print(y_train_original.shape)\n",
    "print(y_test_original.shape)\n",
    "\n",
    "print('rel/unrel')\n",
    "print(y_train_sentence.shape)\n",
    "print(y_test_sentence.shape)\n",
    "\n",
    "print('np sentence')\n",
    "print(y_train_np_sentence.shape)\n",
    "print(y_test_np_sentence.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train A [rel,unrel] classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------\n",
      "C=1.00\n",
      "--------------\n",
      "Accuracy\n",
      "Train:\t0.90623 \n",
      "Test:\t0.74813 \n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.79      0.73      0.76       363\n",
      "        1.0       0.71      0.77      0.74       304\n",
      "\n",
      "avg / total       0.75      0.75      0.75       667\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Okay... Using the function makes me more overwhelmed. Let's do it manually.\n",
    "\n",
    "\n",
    "random_state = 42\n",
    "C = 1\n",
    "\n",
    "clf_A = LogisticRegression(random_state=random_state, C=C)\n",
    "clf_A.fit(X_train_sentence_bow, y_train_sentence)\n",
    "\n",
    "y_predict = clf_A.predict(X_test_sentence_bow)\n",
    "\n",
    "print('--------------')\n",
    "print('C=%.2f' %(C))\n",
    "print('--------------')\n",
    "print('Accuracy')\n",
    "print('Train:\\t%.5f ' %(clf_A.score(X_train_sentence_bow, y_train_sentence)))\n",
    "print('Test:\\t%.5f ' %(clf_A.score(X_test_sentence_bow, y_test_sentence)))\n",
    "    \n",
    "print(classification_report(y_test_sentence,y_predict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train 1 [+,-] classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=42, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# using whole corpus\n",
    "clf_1_i = LogisticRegression(random_state=random_state, C=C)\n",
    "clf_1_i.fit(X_train_original_bow, y_train_original)\n",
    "\n",
    "# using the [+/-] sentence\n",
    "\n",
    "clf_1_j = LogisticRegression(random_state=random_state, C=C)\n",
    "clf_1_j.fit(X_train_np_bow, y_train_np_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------\n",
      "C=1.00\n",
      "--------------\n",
      "Accuracy\n",
      "Train:\t0.84776 \n",
      "Test:\t0.84204 \n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.85      0.83      0.84     12500\n",
      "          1       0.83      0.86      0.84     12500\n",
      "\n",
      "avg / total       0.84      0.84      0.84     25000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Test baseline\n",
    "\n",
    "y_predict = clf_1_i.predict(X_test_original_bow)\n",
    "\n",
    "print('--------------')\n",
    "print('C=%.2f' %(C))\n",
    "print('--------------')\n",
    "print('Accuracy')\n",
    "print('Train:\\t%.5f ' %(clf_1_i.score(X_train_original_bow, y_train_original)))\n",
    "print('Test:\\t%.5f ' %(clf_1_i.score(X_test_original_bow, y_test_original)))\n",
    "    \n",
    "print(classification_report(y_test_original,y_predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------\n",
      "C=1.00\n",
      "--------------\n",
      "Accuracy\n",
      "Train:\t0.94144 \n",
      "Test:\t0.74551 \n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.76      0.69      0.73       163\n",
      "        1.0       0.73      0.80      0.76       171\n",
      "\n",
      "avg / total       0.75      0.75      0.74       334\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Test baseline\n",
    "\n",
    "y_predict = clf_1_j.predict(X_test_np_bow)\n",
    "\n",
    "print('--------------')\n",
    "print('C=%.2f' %(C))\n",
    "print('--------------')\n",
    "print('Accuracy')\n",
    "print('Train:\\t%.5f ' %(clf_1_j.score(X_train_np_bow, y_train_np_sentence)))\n",
    "print('Test:\\t%.5f ' %(clf_1_j.score(X_test_np_bow, y_test_np_sentence)))\n",
    "    \n",
    "print(classification_report(y_test_np_sentence,y_predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1 -1 -1 ... -1 -1 -1]\n",
      "[-1 -1 -1 ... -1 -1 -1]\n"
     ]
    }
   ],
   "source": [
    "# Build an accuracy function excluding those -1\n",
    "\n",
    "y_pred_i = []\n",
    "y_pred_j = []\n",
    "threshold=0.99\n",
    "\n",
    "for ind, corpus in enumerate(X_test_original):\n",
    "    '''\n",
    "    Breakdown the corpus into sentence and transform into bag-of-words\n",
    "    '''\n",
    "    sentence_set = tf_vectorizer.transform(TextBlob(corpus).raw_sentences)\n",
    "    \n",
    "    '''\n",
    "    Related classifier given threshold. \n",
    "    threshold, if None, it only predict the label. If float number given, assign the threshold to the sentence with\n",
    "    probability over threshold\n",
    "    '''\n",
    "    if threshold==None:\n",
    "        y_A_proba = clf_A.predict_proba(sentence_set)\n",
    "        mu, mr = np.argmax(y_A_proba, axis=0)\n",
    "    \n",
    "        if y_A_proba[mr,1] > 0.5:\n",
    "            y_i_proba = clf_1_i.predict_proba(sentence_set[mr])\n",
    "            y_pred_i.append(np.argmax(y_i_proba))\n",
    "            \n",
    "            y_j_proba = clf_1_j.predict_proba(sentence_set[mr])\n",
    "            y_pred_j.append(np.argmax(y_j_proba))\n",
    "        else:\n",
    "            y_pred_i.append(-1)\n",
    "            y_pred_j.append(-1)\n",
    "            continue\n",
    "    else:\n",
    "        y_A_proba = clf_A.predict_proba(sentence_set)\n",
    "        y_threshold = (y_A_proba[:,1] > threshold)\n",
    "        \n",
    "        mr = np.array(np.where(y_threshold)).flatten()\n",
    "        \n",
    "        if len(mr) == 0:\n",
    "            y_pred_i.append(-1)\n",
    "            y_pred_j.append(-1)\n",
    "            continue  \n",
    "        else: \n",
    "            y_i_proba = clf_1_i.predict_proba(sentence_set[mr])\n",
    "            y_i_avg = np.mean(y_i_proba, axis=0)\n",
    "            y_pred_i.append(np.argmax(y_i_avg))\n",
    "            \n",
    "            y_j_proba = clf_1_j.predict_proba(sentence_set[mr])\n",
    "            y_j_avg = np.mean(y_j_proba, axis=0)\n",
    "            y_pred_j.append(np.argmax(y_j_avg))\n",
    "\n",
    "     \n",
    "    \n",
    "y_pred_i = np.array(y_pred_i)\n",
    "y_pred_j = np.array(y_pred_j)\n",
    "\n",
    "print(y_pred_i)\n",
    "print(y_pred_j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t \t ~rel \t rr \t acc_i \t acc_j\n",
      "0.99 \t 18894 \t 0.75576 \t 0.78431 \t 0.71258\n"
     ]
    }
   ],
   "source": [
    "#Procedure call\n",
    "\n",
    "print('t \\t ~rel \\t rr \\t acc_i \\t acc_j')\n",
    "print('%.2f \\t %d \\t %.5f \\t %.5f \\t %.5f' %(threshold,\n",
    "                                       np.sum(y_pred_i==-1),\n",
    "                                       rejection_rate(y_pred_i), \n",
    "                                       accuracy(y_test_original, y_pred_i), \n",
    "                                       accuracy(y_test_original,y_pred_j)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rejection_rate(y):\n",
    "    return np.sum(y==-1)/len(y)\n",
    "\n",
    "def accuracy(y, y_pred):\n",
    "    return np.sum(y_pred==y)/(np.sum(y_pred==1) + np.sum(y_pred==0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt('cos_sim.csv', np.around(cos_sim,2), delimiter=',')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
