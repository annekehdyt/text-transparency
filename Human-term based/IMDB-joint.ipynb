{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from numpy.random import seed\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow import set_random_seed\n",
    "seed(42)\n",
    "set_random_seed(42)\n",
    "\n",
    "from keras.layers import Input, Dense, TimeDistributed, Embedding\n",
    "from keras.layers import Concatenate, Reshape, Lambda, Multiply, multiply, concatenate\n",
    "from keras.models import Model\n",
    "from keras import backend as K\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import style\n",
    "\n",
    "style.use('seaborn-whitegrid')\n",
    "\n",
    "def open_pickle(path):\n",
    "    import pickle\n",
    "    with open(path, 'rb') as f:\n",
    "        X = pickle.load(f)\n",
    "    return X\n",
    "\n",
    "def load_unigrams(path, X, y):\n",
    "    word_list = []\n",
    "    connotation = {}\n",
    "    \n",
    "    with open(path, 'r', encoding='utf8') as f:\n",
    "        for line in f:\n",
    "            word_list.append(line.strip())\n",
    "            \n",
    "    for word in word_list:\n",
    "        pos_count = 0\n",
    "        neg_count = 0\n",
    "        for i, doc in enumerate(X):\n",
    "            if word in doc.lower():\n",
    "                \n",
    "                if (y[i] == 1):\n",
    "                    pos_count += 1\n",
    "                else:\n",
    "                    neg_count += 1\n",
    "                    \n",
    "        if pos_count > neg_count:\n",
    "            connotation[word] = 1\n",
    "        else:\n",
    "            connotation[word] = 0\n",
    "    \n",
    "    return word_list, connotation\n",
    "\n",
    "def generate_appearance(X_train_corpus, X_test_corpus, word_list, connotation):\n",
    "    y_train_agreement = []\n",
    "    for i in range(len(X_train_corpus)):\n",
    "        doc_agreement = []\n",
    "        for word in word_list:\n",
    "            if word in X_train_corpus[i]:\n",
    "                if connotation[word] == 1:\n",
    "                    doc_agreement.append(1)\n",
    "                else:\n",
    "                    doc_agreement.append(-1)\n",
    "            else:\n",
    "                doc_agreement.append(0)\n",
    "        y_train_agreement.append(doc_agreement)\n",
    "        \n",
    "    y_test_agreement = []\n",
    "    for i in range(len(X_test_corpus)):\n",
    "        doc_agreement = []\n",
    "        for word in word_list:\n",
    "            if word in X_test_corpus[i]:\n",
    "                if connotation[word] == 1:\n",
    "                    doc_agreement.append(1)\n",
    "                else:\n",
    "                    doc_agreement.append(-1)\n",
    "            else:\n",
    "                doc_agreement.append(0)\n",
    "        y_test_agreement.append(doc_agreement)\n",
    "        \n",
    "    return np.array(y_train_agreement), np.array(y_test_agreement)\n",
    "\n",
    "# 'imdb-unigrams.txt'\n",
    "\n",
    "X_train_original = open_pickle('../../data/imdb/imdb_original_preprocessed_xtrain.pickle')\n",
    "X_test_original = open_pickle('../../data/imdb/imdb_original_preprocessed_xtest.pickle')\n",
    "y_train_original = open_pickle('../../data/imdb/imdb_original_preprocessed_ytrain.pickle')\n",
    "y_test_original = open_pickle('../../data/imdb/imdb_original_preprocessed_ytest.pickle')\n",
    "\n",
    "# Count vectorizer \n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "token = r\"(?u)\\b[\\w\\'/]+\\b\"\n",
    "cv = CountVectorizer(min_df = 100, token_pattern=token, lowercase=True, binary=True)\n",
    "X_train = cv.fit_transform(X_train_original)\n",
    "X_test = cv.transform(X_test_original)\n",
    "\n",
    "word_list, connotation = load_unigrams('./imdb-unigrams.txt', X_train_original, y_train_original)\n",
    "\n",
    "y_train_agreement, y_test_agreement = generate_appearance(X_train_original, X_test_original, \n",
    "                                                          word_list, connotation)\n",
    "\n",
    "\n",
    "def history_plot(history, model_name):\n",
    "    plt.plot(history.history['acc'])\n",
    "    plt.plot(history.history['val_acc'])\n",
    "        \n",
    "    title = model_name + 'accuracy'\n",
    "    plt.title(title)\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['tr_acc', 'val_acc'], loc='upper left')\n",
    "    plt.show()\n",
    "    plt.clf()\n",
    "\n",
    "    plt.plot(history.history['loss'], 'm--')\n",
    "    plt.plot(history.history['val_loss'], 'y--')\n",
    "\n",
    "    title = model_name + 'loss'\n",
    "    plt.title(title)\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['tr_loss', 'val_loss'], loc='upper left')\n",
    "    plt.show()\n",
    "    plt.clf()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the custom loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # https://stackoverflow.com/questions/48951109/keras-custom-binary-cross-entropy-loss-function-get-nan-as-output-for-loss\n",
    "\n",
    "# def custom_cross_entropy(y_true, y_pred):\n",
    "#     t_loss = K.max(y_pred,0)-y_pred * y_true + K.log(1+K.exp((-1)*K.abs(y_pred)))\n",
    "#     return K.mean(t_loss)\n",
    "\n",
    "# from keras.initializers import Constant, glorot_uniform\n",
    "\n",
    "# input_layer = Input(shape=(X_train.shape[1],))\n",
    "# tanh_output = Dense(1, activation='sigmoid', kernel_initializer=glorot_uniform(seed=42))(input_layer)\n",
    "# model = Model(inputs=input_layer, outputs=tanh_output)\n",
    "\n",
    "# model.compile(loss=custom_cross_entropy,\n",
    "#              metrics=['acc'],\n",
    "#              optimizer='adam')\n",
    "\n",
    "# model.fit(X_train[:16667], y_train_original[:16667], \n",
    "#          validation_data=([X_train[16667:], y_train_original[16667:]]),\n",
    "#          batch_size=1, epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.evaluate(X_test, y_test_original)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.evaluate(X_train, y_train_original)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1st model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.initializers import Constant, glorot_uniform\n",
    "\n",
    "input_layer = Input(shape=(X_train.shape[1],))\n",
    "tanh_output = Dense(1, activation='sigmoid', kernel_initializer=glorot_uniform(seed=42))(input_layer)\n",
    "model = Model(inputs=input_layer, outputs=tanh_output)\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "             metrics=['acc'],\n",
    "             optimizer='adam')\n",
    "\n",
    "# base_history = model.fit(X_train[:16667], y_train_original[:16667], \n",
    "#                  validation_data=([X_train[16667:], y_train_original[16667:]]),\n",
    "#                  batch_size=1, epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.evaluate(X_test, y_test_original)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.evaluate(X_train, y_train_original)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.trainable=True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2nd model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 16667 samples, validate on 8333 samples\n",
      "Epoch 1/50\n",
      "16667/16667 [==============================] - 161s 10ms/step - loss: 0.4724 - acc: 0.7631 - val_loss: 0.4157 - val_acc: 0.7943\n",
      "Epoch 2/50\n",
      "16667/16667 [==============================] - 156s 9ms/step - loss: 0.3554 - acc: 0.8214 - val_loss: 0.4200 - val_acc: 0.7964\n",
      "Epoch 3/50\n",
      "16667/16667 [==============================] - 161s 10ms/step - loss: 0.3256 - acc: 0.8323 - val_loss: 0.4353 - val_acc: 0.7935\n",
      "Epoch 4/50\n",
      "16667/16667 [==============================] - 153s 9ms/step - loss: 0.3075 - acc: 0.8407 - val_loss: 0.4548 - val_acc: 0.7924\n",
      "Epoch 5/50\n",
      "16667/16667 [==============================] - 148s 9ms/step - loss: 0.2944 - acc: 0.8444 - val_loss: 0.4867 - val_acc: 0.7940\n",
      "Epoch 6/50\n",
      "16667/16667 [==============================] - 143s 9ms/step - loss: 0.2857 - acc: 0.8468 - val_loss: 0.5067 - val_acc: 0.7940\n",
      "Epoch 7/50\n",
      "16667/16667 [==============================] - 148s 9ms/step - loss: 0.2795 - acc: 0.8477 - val_loss: 0.5454 - val_acc: 0.7902\n",
      "Epoch 8/50\n",
      "16667/16667 [==============================] - 142s 9ms/step - loss: 0.2700 - acc: 0.8529 - val_loss: 0.5801 - val_acc: 0.7902\n",
      "Epoch 9/50\n",
      "16667/16667 [==============================] - 144s 9ms/step - loss: 0.2669 - acc: 0.8533 - val_loss: 0.5870 - val_acc: 0.7874\n",
      "Epoch 10/50\n",
      "16667/16667 [==============================] - 146s 9ms/step - loss: 0.2624 - acc: 0.8570 - val_loss: 0.6136 - val_acc: 0.7853\n",
      "Epoch 11/50\n",
      "16667/16667 [==============================] - 154s 9ms/step - loss: 0.2592 - acc: 0.8584 - val_loss: 0.6404 - val_acc: 0.7847\n",
      "Epoch 12/50\n",
      "16667/16667 [==============================] - 156s 9ms/step - loss: 0.2570 - acc: 0.8603 - val_loss: 0.6653 - val_acc: 0.7835\n",
      "Epoch 13/50\n",
      "16667/16667 [==============================] - 157s 9ms/step - loss: 0.2564 - acc: 0.8567 - val_loss: 0.6863 - val_acc: 0.7834\n",
      "Epoch 14/50\n",
      "16667/16667 [==============================] - 157s 9ms/step - loss: 0.2518 - acc: 0.8600 - val_loss: 0.7191 - val_acc: 0.7811\n",
      "Epoch 15/50\n",
      "16667/16667 [==============================] - 157s 9ms/step - loss: 0.2511 - acc: 0.8622 - val_loss: 0.7860 - val_acc: 0.7814\n",
      "Epoch 16/50\n",
      "16667/16667 [==============================] - 159s 10ms/step - loss: 0.2475 - acc: 0.8637 - val_loss: 0.7495 - val_acc: 0.7803\n",
      "Epoch 17/50\n",
      "16667/16667 [==============================] - 157s 9ms/step - loss: 0.2501 - acc: 0.8617 - val_loss: 0.7874 - val_acc: 0.7808\n",
      "Epoch 18/50\n",
      "16667/16667 [==============================] - 172s 10ms/step - loss: 0.2498 - acc: 0.8598 - val_loss: 0.8167 - val_acc: 0.7774\n",
      "Epoch 19/50\n",
      "16667/16667 [==============================] - 158s 9ms/step - loss: 0.2435 - acc: 0.8643 - val_loss: 0.8138 - val_acc: 0.7786\n",
      "Epoch 20/50\n",
      "16667/16667 [==============================] - 163s 10ms/step - loss: 0.2410 - acc: 0.8666 - val_loss: 0.8225 - val_acc: 0.7763\n",
      "Epoch 21/50\n",
      "16667/16667 [==============================] - 160s 10ms/step - loss: 0.2411 - acc: 0.8661 - val_loss: 0.8392 - val_acc: 0.7763\n",
      "Epoch 22/50\n",
      "16667/16667 [==============================] - 157s 9ms/step - loss: 0.2383 - acc: 0.8661 - val_loss: 0.8610 - val_acc: 0.7796\n",
      "Epoch 23/50\n",
      "16667/16667 [==============================] - 158s 9ms/step - loss: 0.2413 - acc: 0.8660 - val_loss: 0.8766 - val_acc: 0.7787\n",
      "Epoch 24/50\n",
      "16667/16667 [==============================] - 158s 9ms/step - loss: 0.2399 - acc: 0.8676 - val_loss: 0.8916 - val_acc: 0.7769\n",
      "Epoch 25/50\n",
      "16667/16667 [==============================] - 158s 9ms/step - loss: 0.2414 - acc: 0.8658 - val_loss: 0.9064 - val_acc: 0.7780\n",
      "Epoch 26/50\n",
      "16667/16667 [==============================] - 157s 9ms/step - loss: 0.2359 - acc: 0.8692 - val_loss: 0.9462 - val_acc: 0.7760\n",
      "Epoch 27/50\n",
      "16667/16667 [==============================] - 158s 9ms/step - loss: 0.2404 - acc: 0.8677 - val_loss: 0.9591 - val_acc: 0.7776\n",
      "Epoch 28/50\n",
      "16667/16667 [==============================] - 163s 10ms/step - loss: 0.2407 - acc: 0.8678 - val_loss: 0.9951 - val_acc: 0.7725\n",
      "Epoch 29/50\n",
      "16667/16667 [==============================] - 157s 9ms/step - loss: 0.2412 - acc: 0.8682 - val_loss: 1.0027 - val_acc: 0.7718\n",
      "Epoch 30/50\n",
      "16667/16667 [==============================] - 161s 10ms/step - loss: 0.2547 - acc: 0.8631 - val_loss: 1.0083 - val_acc: 0.7763\n",
      "Epoch 31/50\n",
      "16667/16667 [==============================] - 154s 9ms/step - loss: 0.2463 - acc: 0.8624 - val_loss: 1.0150 - val_acc: 0.7725\n",
      "Epoch 32/50\n",
      "16667/16667 [==============================] - 154s 9ms/step - loss: 0.2404 - acc: 0.8646 - val_loss: 1.0104 - val_acc: 0.7736\n",
      "Epoch 33/50\n",
      "16667/16667 [==============================] - 154s 9ms/step - loss: 0.2407 - acc: 0.8679 - val_loss: 1.0430 - val_acc: 0.7714\n",
      "Epoch 34/50\n",
      "16667/16667 [==============================] - 157s 9ms/step - loss: 0.2381 - acc: 0.8678 - val_loss: 1.0491 - val_acc: 0.7718\n",
      "Epoch 35/50\n",
      "16667/16667 [==============================] - 160s 10ms/step - loss: 0.2409 - acc: 0.8683 - val_loss: 1.0681 - val_acc: 0.7704\n",
      "Epoch 36/50\n",
      "10995/16667 [==================>...........] - ETA: 48s - loss: 0.2379 - acc: 0.8643"
     ]
    }
   ],
   "source": [
    "def layer_split(x):\n",
    "    return tf.split(x,num_or_size_splits=human_terms_len,axis=1)\n",
    "\n",
    "def layer_concat(x):\n",
    "    return tf.concat(x, axis=1)\n",
    "\n",
    "# build the combined model\n",
    "# Combined model\n",
    "human_terms_len = len(word_list)\n",
    "\n",
    "# base_model = build_base_model(X_train.shape[1])\n",
    "\n",
    "combined_input_layer = Input(shape=(X_train.shape[1],))\n",
    "\n",
    "# build the hard coded weight for human terms\n",
    "ht_input_layer = Input(shape=(human_terms_len,))\n",
    "\n",
    "# split = Lambda( lambda x: tf.split(x,num_or_size_splits=human_terms_len,axis=1))(ht_input_layer)\n",
    "split = Lambda(layer_split)(ht_input_layer)\n",
    "\n",
    "# get the document prediction\n",
    "label_layer = model(combined_input_layer)\n",
    "tanh_norm = Lambda(lambda x: (x*2)-1)(label_layer)\n",
    "# tanh_norm = Lambda(lambda x: tf.scalar_mul(2,x)-1)(label_layer)\n",
    "\n",
    "# do normalize of bipolar sigmoid\n",
    "\n",
    "\n",
    "# stack the multiply layer\n",
    "dense_layer = []\n",
    "for i in range(human_terms_len):\n",
    "    dense_layer.append(Dense(1, activation='relu', use_bias=False)(Multiply()([split[i], tanh_norm])))\n",
    "\n",
    "# concat all the result   \n",
    "# concat = Lambda( lambda x: tf.concat(x, axis=1), name='concatenate')(dense_layer)\n",
    "concat = Lambda(layer_concat, name='concatenate')(dense_layer)\n",
    "\n",
    "\n",
    "# pass it to sigmoid layer\n",
    "output_layer = Dense(1, activation='sigmoid')(concat)\n",
    "\n",
    "combined_model = Model(inputs=[combined_input_layer, ht_input_layer], outputs=output_layer)\n",
    "# combined_model.summary()\n",
    "\n",
    "\n",
    "combined_model.compile(loss='binary_crossentropy',\n",
    "                      optimizer='adam',\n",
    "                      metrics=['acc'])\n",
    "\n",
    "# y_train_tanh = y_train_original\n",
    "# y_train_tanh[y_train_tanh == 0] = -1\n",
    "\n",
    "# y_test_tanh = y_test_original\n",
    "# y_test_tanh[y_test_tanh == 0] = -1\n",
    "\n",
    "# base_model_history = base_model.fit(X_train[:16667], y_train_original[:16667], \n",
    "#                                     validation_data=(X_train[16667:], y_train_original[16667:]),\n",
    "#                                     batch_size=1, epochs=1)\n",
    "\n",
    "combined_model_history = combined_model.fit([X_train[:16667],y_train_agreement[:16667]], y_train_original[:16667], \n",
    "                                            validation_data=([X_train[16667:], y_train_agreement[16667:]], y_train_original[16667:]),\n",
    "                                            batch_size=1, epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_model.evaluate([X_test, y_test_agreement], y_test_original)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_model.evaluate([X_train, y_train_agreement], y_train_original)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy_reject(combined_model, X, y_agreement, y):\n",
    "    human_terms_relu_model = Model(inputs=combined_model.input,\n",
    "                                    outputs=combined_model.get_layer('concatenate').output)\n",
    "    predict_relu = human_terms_relu_model.predict([X, y_agreement])\n",
    "    accept_indices = np.where(np.sum(predict_relu, axis=1)!=0)\n",
    "    accept_indices = accept_indices[0]\n",
    "    total_reject = X.shape[0] - len(accept_indices)\n",
    "    rejection_rate = total_reject/X.shape[0]\n",
    "\n",
    "    test_eval = combined_model.evaluate([X[accept_indices], y_agreement[accept_indices]], y[accept_indices])\n",
    "    \n",
    "    return test_eval, rejection_rate, total_reject"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_reject(combined_model, X_test, y_test_agreement, y_test_original)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_reject(combined_model, X_train, y_train_agreement, y_train_original)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ev = combined_model.evaluate([X_test, y_test_agreement], y_test_original)\n",
    "train_ev = combined_model.evaluate([X_train, y_train_agreement], y_train_original)\n",
    "\n",
    "def accuracy_reject(combined_model, X, y_agreement, y):\n",
    "    human_terms_relu_model = Model(inputs=combined_model.input,\n",
    "                                    outputs=combined_model.get_layer('concatenate').output)\n",
    "    predict_relu = human_terms_relu_model.predict([X, y_agreement])\n",
    "    accept_indices = np.where(np.sum(predict_relu, axis=1)!=0)\n",
    "    accept_indices = accept_indices[0]\n",
    "    total_reject = X.shape[0] - len(accept_indices)\n",
    "    rejection_rate = total_reject/X.shape[0]\n",
    "\n",
    "    test_eval = combined_model.evaluate([X[accept_indices], y_agreement[accept_indices]], y[accept_indices])\n",
    "    \n",
    "    return test_eval, rejection_rate, total_reject\n",
    "\n",
    "test_ev_reject = accuracy_reject(combined_model, X_test, y_test_agreement, y_test_original)\n",
    "train_ev_reject = accuracy_reject(combined_model, X_train, y_train_agreement, y_train_original)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('%f \\t %f' %(train_ev[0], train_ev[1]))\n",
    "print('%f \\t %f' %(test_ev[0], test_ev[1]))\n",
    "print('%f \\t %f \\t %f \\t %d' %(train_ev_reject[0][0], train_ev_reject[0][1], train_ev_reject[1], train_ev_reject[2]))\n",
    "print('%f \\t %f \\t %f \\t %d' %(test_ev_reject[0][0], test_ev_reject[0][1], test_ev_reject[1], test_ev_reject[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# history_plot(base_history,'RDclf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_plot(combined_model_history,'TTclf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_model.save('./figure/stable-imdb-50-50-joint.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
