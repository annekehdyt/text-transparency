{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from numpy.random import seed\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow import set_random_seed\n",
    "seed(42)\n",
    "set_random_seed(42)\n",
    "\n",
    "from keras.layers import Input, Dense, TimeDistributed, Embedding\n",
    "from keras.layers import Concatenate, Reshape, Lambda, Multiply, multiply, concatenate\n",
    "from keras.models import Model\n",
    "from keras import backend as K\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import style\n",
    "\n",
    "style.use('seaborn-whitegrid')\n",
    "\n",
    "def open_pickle(path):\n",
    "    import pickle\n",
    "    with open(path, 'rb') as f:\n",
    "        X = pickle.load(f)\n",
    "    return X\n",
    "\n",
    "def load_unigrams(path, X, y):\n",
    "    word_list = []\n",
    "    connotation = {}\n",
    "    \n",
    "    with open(path, 'r', encoding='utf8') as f:\n",
    "        for line in f:\n",
    "            word_list.append(line.strip())\n",
    "            \n",
    "    for word in word_list:\n",
    "        pos_count = 0\n",
    "        neg_count = 0\n",
    "        for i, doc in enumerate(X):\n",
    "            if word in doc.lower():\n",
    "                \n",
    "                if (y[i] == 1):\n",
    "                    pos_count += 1\n",
    "                else:\n",
    "                    neg_count += 1\n",
    "                    \n",
    "        if pos_count > neg_count:\n",
    "            connotation[word] = 1\n",
    "        else:\n",
    "            connotation[word] = 0\n",
    "    \n",
    "    return word_list, connotation\n",
    "\n",
    "def generate_appearance(X_train_corpus, X_test_corpus, word_list, connotation):\n",
    "    y_train_agreement = []\n",
    "    for i in range(len(X_train_corpus)):\n",
    "        doc_agreement = []\n",
    "        for word in word_list:\n",
    "            if word in X_train_corpus[i]:\n",
    "                if connotation[word] == 1:\n",
    "                    doc_agreement.append(1)\n",
    "                else:\n",
    "                    doc_agreement.append(-1)\n",
    "            else:\n",
    "                doc_agreement.append(0)\n",
    "        y_train_agreement.append(doc_agreement)\n",
    "        \n",
    "    y_test_agreement = []\n",
    "    for i in range(len(X_test_corpus)):\n",
    "        doc_agreement = []\n",
    "        for word in word_list:\n",
    "            if word in X_test_corpus[i]:\n",
    "                if connotation[word] == 1:\n",
    "                    doc_agreement.append(1)\n",
    "                else:\n",
    "                    doc_agreement.append(-1)\n",
    "            else:\n",
    "                doc_agreement.append(0)\n",
    "        y_test_agreement.append(doc_agreement)\n",
    "        \n",
    "    return np.array(y_train_agreement), np.array(y_test_agreement)\n",
    "\n",
    "# 'imdb-unigrams.txt'\n",
    "\n",
    "X_train_original = open_pickle('../../data/imdb/imdb_original_preprocessed_xtrain.pickle')\n",
    "X_test_original = open_pickle('../../data/imdb/imdb_original_preprocessed_xtest.pickle')\n",
    "y_train_original = open_pickle('../../data/imdb/imdb_original_preprocessed_ytrain.pickle')\n",
    "y_test_original = open_pickle('../../data/imdb/imdb_original_preprocessed_ytest.pickle')\n",
    "\n",
    "# Count vectorizer \n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "token = r\"(?u)\\b[\\w\\'/]+\\b\"\n",
    "cv = CountVectorizer(min_df = 100, token_pattern=token, lowercase=True, binary=True)\n",
    "X_train = cv.fit_transform(X_train_original)\n",
    "X_test = cv.transform(X_test_original)\n",
    "\n",
    "word_list, connotation = load_unigrams('./imdb-unigrams.txt', X_train_original, y_train_original)\n",
    "\n",
    "y_train_agreement, y_test_agreement = generate_appearance(X_train_original, X_test_original, \n",
    "                                                          word_list, connotation)\n",
    "\n",
    "\n",
    "def history_plot(history, model_name):\n",
    "    plt.plot(history.history['acc'])\n",
    "    plt.plot(history.history['val_acc'])\n",
    "        \n",
    "    title = model_name + 'accuracy'\n",
    "    plt.title(title)\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['tr_acc', 'val_acc'], loc='upper left')\n",
    "    plt.show()\n",
    "    plt.clf()\n",
    "\n",
    "    plt.plot(history.history['loss'], 'm--')\n",
    "    plt.plot(history.history['val_loss'], 'y--')\n",
    "\n",
    "    title = model_name + 'loss'\n",
    "    plt.title(title)\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['tr_loss', 'val_loss'], loc='upper left')\n",
    "    plt.show()\n",
    "    plt.clf()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the custom loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # https://stackoverflow.com/questions/48951109/keras-custom-binary-cross-entropy-loss-function-get-nan-as-output-for-loss\n",
    "\n",
    "# def custom_cross_entropy(y_true, y_pred):\n",
    "#     t_loss = K.max(y_pred,0)-y_pred * y_true + K.log(1+K.exp((-1)*K.abs(y_pred)))\n",
    "#     return K.mean(t_loss)\n",
    "\n",
    "# from keras.initializers import Constant, glorot_uniform\n",
    "\n",
    "# input_layer = Input(shape=(X_train.shape[1],))\n",
    "# tanh_output = Dense(1, activation='sigmoid', kernel_initializer=glorot_uniform(seed=42))(input_layer)\n",
    "# model = Model(inputs=input_layer, outputs=tanh_output)\n",
    "\n",
    "# model.compile(loss=custom_cross_entropy,\n",
    "#              metrics=['acc'],\n",
    "#              optimizer='adam')\n",
    "\n",
    "# model.fit(X_train[:16667], y_train_original[:16667], \n",
    "#          validation_data=([X_train[16667:], y_train_original[16667:]]),\n",
    "#          batch_size=1, epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.evaluate(X_test, y_test_original)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.evaluate(X_train, y_train_original)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1st model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 16667 samples, validate on 8333 samples\n",
      "Epoch 1/1\n",
      "16667/16667 [==============================] - 42s 3ms/step - loss: 0.3437 - acc: 0.8613 - val_loss: 0.3015 - val_acc: 0.8766\n"
     ]
    }
   ],
   "source": [
    "from keras.initializers import Constant, glorot_uniform\n",
    "\n",
    "input_layer = Input(shape=(X_train.shape[1],))\n",
    "tanh_output = Dense(1, activation='sigmoid', kernel_initializer=glorot_uniform(seed=42))(input_layer)\n",
    "model = Model(inputs=input_layer, outputs=tanh_output)\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "             metrics=['acc'],\n",
    "             optimizer='adam')\n",
    "\n",
    "base_history = model.fit(X_train[:16667], y_train_original[:16667], \n",
    "                 validation_data=([X_train[16667:], y_train_original[16667:]]),\n",
    "                 batch_size=1, epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000/25000 [==============================] - 1s 56us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.28975145785331724, 0.88192]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_test, y_test_original)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000/25000 [==============================] - 1s 55us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.24867058249235152, 0.90656]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_train, y_train_original)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.trainable=True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2nd model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 16667 samples, validate on 8333 samples\n",
      "Epoch 1/50\n",
      "16667/16667 [==============================] - 159s 10ms/step - loss: 0.3857 - acc: 0.8297 - val_loss: 0.3907 - val_acc: 0.8218\n",
      "Epoch 2/50\n",
      "16667/16667 [==============================] - 155s 9ms/step - loss: 0.3047 - acc: 0.8514 - val_loss: 0.4094 - val_acc: 0.8192\n",
      "Epoch 3/50\n",
      "16667/16667 [==============================] - 154s 9ms/step - loss: 0.2811 - acc: 0.8611 - val_loss: 0.4617 - val_acc: 0.8097\n",
      "Epoch 4/50\n",
      "16667/16667 [==============================] - 154s 9ms/step - loss: 0.2691 - acc: 0.8619 - val_loss: 0.4720 - val_acc: 0.8038\n",
      "Epoch 5/50\n",
      "16667/16667 [==============================] - 156s 9ms/step - loss: 0.2610 - acc: 0.8678 - val_loss: 0.5134 - val_acc: 0.8045\n",
      "Epoch 6/50\n",
      "16667/16667 [==============================] - 155s 9ms/step - loss: 0.2524 - acc: 0.8673 - val_loss: 0.5337 - val_acc: 0.8090\n",
      "Epoch 7/50\n",
      "16667/16667 [==============================] - 157s 9ms/step - loss: 0.2424 - acc: 0.8720 - val_loss: 0.5555 - val_acc: 0.8117\n",
      "Epoch 8/50\n",
      "16667/16667 [==============================] - 167s 10ms/step - loss: 0.2390 - acc: 0.8741 - val_loss: 0.6005 - val_acc: 0.8093\n",
      "Epoch 9/50\n",
      "16667/16667 [==============================] - 159s 10ms/step - loss: 0.2340 - acc: 0.8730 - val_loss: 0.6194 - val_acc: 0.7998\n",
      "Epoch 10/50\n",
      "16667/16667 [==============================] - 159s 10ms/step - loss: 0.2299 - acc: 0.8786 - val_loss: 0.6694 - val_acc: 0.8016\n",
      "Epoch 11/50\n",
      "16667/16667 [==============================] - 155s 9ms/step - loss: 0.2270 - acc: 0.8786 - val_loss: 0.7225 - val_acc: 0.8001\n",
      "Epoch 12/50\n",
      "16667/16667 [==============================] - 155s 9ms/step - loss: 0.2258 - acc: 0.8793 - val_loss: 0.7244 - val_acc: 0.7935\n",
      "Epoch 13/50\n",
      "16667/16667 [==============================] - 155s 9ms/step - loss: 0.2257 - acc: 0.8832 - val_loss: 0.7621 - val_acc: 0.8014\n",
      "Epoch 14/50\n",
      "16667/16667 [==============================] - 156s 9ms/step - loss: 0.2234 - acc: 0.8816 - val_loss: 0.7654 - val_acc: 0.8048\n",
      "Epoch 15/50\n",
      "16667/16667 [==============================] - 155s 9ms/step - loss: 0.2221 - acc: 0.8790 - val_loss: 0.7870 - val_acc: 0.7962\n",
      "Epoch 16/50\n",
      "16667/16667 [==============================] - 155s 9ms/step - loss: 0.2237 - acc: 0.8858 - val_loss: 0.8169 - val_acc: 0.8026\n",
      "Epoch 17/50\n",
      "16667/16667 [==============================] - 156s 9ms/step - loss: 0.2162 - acc: 0.8808 - val_loss: 0.8434 - val_acc: 0.8031\n",
      "Epoch 18/50\n",
      "16667/16667 [==============================] - 160s 10ms/step - loss: 0.2171 - acc: 0.8847 - val_loss: 0.8738 - val_acc: 0.8022\n",
      "Epoch 19/50\n",
      "16667/16667 [==============================] - 156s 9ms/step - loss: 0.2158 - acc: 0.8811 - val_loss: 0.8815 - val_acc: 0.8020\n",
      "Epoch 20/50\n",
      "16667/16667 [==============================] - 156s 9ms/step - loss: 0.2102 - acc: 0.8856 - val_loss: 0.9129 - val_acc: 0.8013\n",
      "Epoch 21/50\n",
      "16667/16667 [==============================] - 152s 9ms/step - loss: 0.2151 - acc: 0.8855 - val_loss: 0.9239 - val_acc: 0.8014\n",
      "Epoch 22/50\n",
      "16667/16667 [==============================] - 152s 9ms/step - loss: 0.2130 - acc: 0.8854 - val_loss: 0.9413 - val_acc: 0.7869\n",
      "Epoch 23/50\n",
      "16667/16667 [==============================] - 153s 9ms/step - loss: 0.2133 - acc: 0.8846 - val_loss: 0.9693 - val_acc: 0.7886\n",
      "Epoch 24/50\n",
      "16667/16667 [==============================] - 154s 9ms/step - loss: 0.2190 - acc: 0.8816 - val_loss: 0.9753 - val_acc: 0.7876\n",
      "Epoch 25/50\n",
      "16667/16667 [==============================] - 159s 10ms/step - loss: 0.2117 - acc: 0.8880 - val_loss: 0.9850 - val_acc: 0.7907\n",
      "Epoch 26/50\n",
      " 8847/16667 [==============>...............] - ETA: 1:06 - loss: 0.1956 - acc: 0.8896"
     ]
    }
   ],
   "source": [
    "def layer_split(x):\n",
    "    return tf.split(x,num_or_size_splits=human_terms_len,axis=1)\n",
    "\n",
    "def layer_concat(x):\n",
    "    return tf.concat(x, axis=1)\n",
    "\n",
    "# build the combined model\n",
    "# Combined model\n",
    "human_terms_len = len(word_list)\n",
    "\n",
    "# base_model = build_base_model(X_train.shape[1])\n",
    "\n",
    "combined_input_layer = Input(shape=(X_train.shape[1],))\n",
    "\n",
    "# build the hard coded weight for human terms\n",
    "ht_input_layer = Input(shape=(human_terms_len,))\n",
    "\n",
    "# split = Lambda( lambda x: tf.split(x,num_or_size_splits=human_terms_len,axis=1))(ht_input_layer)\n",
    "split = Lambda(layer_split)(ht_input_layer)\n",
    "\n",
    "# get the document prediction\n",
    "label_layer = model(combined_input_layer)\n",
    "tanh_norm = Lambda(lambda x: (x*2)-1)(label_layer)\n",
    "# tanh_norm = Lambda(lambda x: tf.scalar_mul(2,x)-1)(label_layer)\n",
    "\n",
    "# do normalize of bipolar sigmoid\n",
    "\n",
    "\n",
    "# stack the multiply layer\n",
    "dense_layer = []\n",
    "for i in range(human_terms_len):\n",
    "    dense_layer.append(Dense(1, activation='relu', use_bias=False)(Multiply()([split[i], tanh_norm])))\n",
    "\n",
    "# concat all the result   \n",
    "# concat = Lambda( lambda x: tf.concat(x, axis=1), name='concatenate')(dense_layer)\n",
    "concat = Lambda(layer_concat, name='concatenate')(dense_layer)\n",
    "\n",
    "\n",
    "# pass it to sigmoid layer\n",
    "output_layer = Dense(1, activation='sigmoid')(concat)\n",
    "\n",
    "combined_model = Model(inputs=[combined_input_layer, ht_input_layer], outputs=output_layer)\n",
    "# combined_model.summary()\n",
    "\n",
    "\n",
    "combined_model.compile(loss='binary_crossentropy',\n",
    "                      optimizer='adam',\n",
    "                      metrics=['acc'])\n",
    "\n",
    "# y_train_tanh = y_train_original\n",
    "# y_train_tanh[y_train_tanh == 0] = -1\n",
    "\n",
    "# y_test_tanh = y_test_original\n",
    "# y_test_tanh[y_test_tanh == 0] = -1\n",
    "\n",
    "# base_model_history = base_model.fit(X_train[:16667], y_train_original[:16667], \n",
    "#                                     validation_data=(X_train[16667:], y_train_original[16667:]),\n",
    "#                                     batch_size=1, epochs=1)\n",
    "\n",
    "combined_model_history = combined_model.fit([X_train[:16667],y_train_agreement[:16667]], y_train_original[:16667], \n",
    "                                            validation_data=([X_train[16667:], y_train_agreement[16667:]], y_train_original[16667:]),\n",
    "                                            batch_size=1, epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_model.evaluate([X_test, y_test_agreement], y_test_original)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_model.evaluate([X_train, y_train_agreement], y_train_original)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ev = combined_model.evaluate([X_test, y_test_agreement], y_test_original)\n",
    "train_ev = combined_model.evaluate([X_train, y_train_agreement], y_train_original)\n",
    "\n",
    "def accuracy_reject(combined_model, X, y_agreement, y):\n",
    "    human_terms_relu_model = Model(inputs=combined_model.input,\n",
    "                                    outputs=combined_model.get_layer('concatenate').output)\n",
    "    predict_relu = human_terms_relu_model.predict([X, y_agreement])\n",
    "    accept_indices = np.where(np.sum(predict_relu, axis=1)!=0)\n",
    "    accept_indices = accept_indices[0]\n",
    "    total_reject = X.shape[0] - len(accept_indices)\n",
    "    rejection_rate = total_reject/X.shape[0]\n",
    "\n",
    "    test_eval = combined_model.evaluate([X[accept_indices], y_agreement[accept_indices]], y[accept_indices])\n",
    "    \n",
    "    return test_eval, rejection_rate, total_reject\n",
    "\n",
    "test_ev_reject = accuracy_reject(combined_model, X_test, y_test_agreement, y_test_original)\n",
    "train_ev_reject = accuracy_reject(combined_model, X_train, y_train_agreement, y_train_original)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('%f \\t %f' %(train_ev[0], train_ev[1]))\n",
    "print('%f \\t %f' %(test_ev[0], test_ev[1]))\n",
    "print('%f \\t %f \\t %f \\t %d' %(train_ev_reject[0][0], train_ev_reject[0][1], train_ev_reject[1], train_ev_reject[2]))\n",
    "print('%f \\t %f \\t %f \\t %d' %(test_ev_reject[0][0], test_ev_reject[0][1], test_ev_reject[1], test_ev_reject[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_plot(base_history,'RDclf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_plot(combined_model_history,'TTclf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('./figure/stable-imdb-trainable-1-50/stable-imdb-trainable-1-50-base.h5')\n",
    "combined_model.save('./figure/stable-imdb-trainable-1-50/stable-imdb-trainable-1-50-combined.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
