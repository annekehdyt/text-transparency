{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from dataset_load import *\n",
    "\n",
    "X_train_original = open_pickle('../../data/imdb/imdb_original_preprocessed_xtrain.pickle')\n",
    "X_test_original = open_pickle('../../data/imdb/imdb_original_preprocessed_xtest.pickle')\n",
    "y_train_original = open_pickle('../../data/imdb/imdb_original_preprocessed_ytrain.pickle')\n",
    "y_test_original = open_pickle('../../data/imdb/imdb_original_preprocessed_ytest.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################################################\n",
    "\n",
    "# Count vectorizer \n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "token = r\"(?u)\\b[\\w\\'/]+\\b\"\n",
    "cv = CountVectorizer(min_df = 100, token_pattern=token, lowercase=True, binary=True)\n",
    "X_train = cv.fit_transform(X_train_original)\n",
    "X_test = cv.transform(X_test_original)\n",
    "\n",
    "######################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3686"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(cv.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LR1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l1', random_state=42, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "lr1 = LogisticRegression(penalty='l1', random_state=42)\n",
    "lr1.fit(X_train, y_train_original)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LR2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_list, connotation = load_unigrams('./imdb-unigrams.txt', X_train_original, y_train_original)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_agreement, y_test_agreement = generate_appearance(X_train_original, X_test_original, \n",
    "                                                          word_list, connotation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "htcv = CountVectorizer(min_df = 100, token_pattern=token, lowercase=True, binary=True, vocabulary=word_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "ht_train = htcv.transform(X_train_original)\n",
    "ht_test = htcv.transform(X_test_original)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_agreement_pos = np.copy(y_train_agreement)\n",
    "y_train_agreement_neg = np.copy(y_train_agreement)\n",
    "y_test_agreement_pos = np.copy(y_test_agreement)\n",
    "y_test_agreement_neg = np.copy(y_test_agreement)\n",
    "\n",
    "y_train_agreement_pos[y_train_agreement_pos == -1] = 0\n",
    "y_train_agreement_neg[y_train_agreement_neg == 1] = 0\n",
    "y_test_agreement_pos[y_test_agreement_pos == -1] = 0\n",
    "y_test_agreement_neg[y_test_agreement_neg == 1] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### From actual label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr2_train = list()\n",
    "for i,y in enumerate(y_train_original):\n",
    "    if y == 1:\n",
    "        lr2_train.append(y_train_agreement_pos[i] * y)\n",
    "    else:\n",
    "        lr2_train.append(y_train_agreement_neg[i] * y)\n",
    "        \n",
    "lr2_train = np.asarray(lr2_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l1', random_state=42, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr2 = LogisticRegression(penalty='l1', random_state=42)\n",
    "lr2.fit(lr2_train, y_train_original)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8418\n",
      "0.8418\n"
     ]
    }
   ],
   "source": [
    "# Test with predict {0,1}\n",
    "\n",
    "lr1_predict = lr1.predict(X_test)\n",
    "\n",
    "lr2_test_bin = list()\n",
    "for i,y in enumerate(lr1_predict):\n",
    "    if y==1:\n",
    "        lr2_test_bin.append(y_test_agreement_pos[i] * y)\n",
    "    else:\n",
    "        lr2_test_bin.append(y_test_agreement_neg[i] * y)\n",
    "    \n",
    "lr2_test_bin = np.asarray(lr2_test_bin)\n",
    "\n",
    "print(lr2.score(lr2_test_bin, y_test_original))\n",
    "\n",
    "# Test with predict [0,1]\n",
    "\n",
    "lr1_predict = lr1.predict_proba(X_test)[:,1]\n",
    "\n",
    "lr2_test_proba = list()\n",
    "for i,y in enumerate(lr1_predict):\n",
    "    if y>=0.5:\n",
    "        lr2_test_proba.append(y_test_agreement_pos[i] * y)\n",
    "    else:\n",
    "        lr2_test_proba.append(y_test_agreement_neg[i] * y)\n",
    "    \n",
    "lr2_test_proba = np.asarray(lr2_test_proba)\n",
    "\n",
    "print(lr2.score(lr2_test_proba, y_test_original))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.        , 0.        , 0.        , 0.        , 3.60950486,\n",
       "       2.00050248, 5.57702426, 5.42721375, 4.47975806, 5.48895803,\n",
       "       6.49374532, 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 7.12266294, 0.        , 7.99754779, 0.        ,\n",
       "       0.        , 4.29804201, 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 5.78204372, 6.57889363,\n",
       "       7.16281132, 0.        , 5.52208984, 5.79987759, 6.5694834 ,\n",
       "       0.        , 8.08820867, 0.        , 0.        , 5.73227543,\n",
       "       8.40122709, 0.        , 5.4907169 , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 6.49429474, 0.        ,\n",
       "       0.        , 0.        , 5.54924019, 0.        , 0.        ,\n",
       "       6.74646851, 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 5.70928471, 5.21552235, 0.        , 4.56295182,\n",
       "       0.        , 0.        , 5.29493477, 0.        , 5.63213107,\n",
       "       5.92644426, 5.25289555, 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 6.9408705 ,\n",
       "       0.        , 0.        , 0.        ])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weight = lr2.coef_.flatten()\n",
    "weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_weight(clf1, clf2=None):\n",
    "    w = clf1.coef_.flatten()\n",
    "    indices = np.argsort(w)[::-1]\n",
    "    con = list(connotation.values())\n",
    "\n",
    "    if clf2 is not None:\n",
    "        w2 = clf2.coef_.flatten()\n",
    "        for i in indices:\n",
    "            print('%s \\t %.3f \\t %.3f \\t %d' %(word_list[i], w[i], w2[i], con[i]))\n",
    "    else:  \n",
    "        for i in indices:\n",
    "            print('%s \\t %.3f \\t %d' %(word_list[i], w[i],con[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train with predicted label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_y = lr1.predict(X_train)\n",
    "proba_y = lr1.predict_proba(X_train)[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr2_train_bin = list()\n",
    "for i,y in enumerate(predicted_y):\n",
    "    if y == 1:\n",
    "        lr2_train_bin.append(y_train_agreement_pos[i] * y)\n",
    "    else:\n",
    "        lr2_train_bin.append(y_train_agreement_neg[i] * y)\n",
    "        \n",
    "lr2_train_bin = np.asarray(lr2_train_bin)\n",
    "\n",
    "lr2_train_proba = list()\n",
    "for i,y in enumerate(proba_y):\n",
    "    if y >= 0.5:\n",
    "        lr2_train_proba.append(y_train_agreement_pos[i] * y)\n",
    "    else:\n",
    "        lr2_train_proba.append(y_train_agreement_neg[i] * y)\n",
    "        \n",
    "lr2_train_proba = np.asarray(lr2_train_proba)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l1', random_state=42, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train with {0,1} label\n",
    "# Train with [0,1] proba\n",
    "\n",
    "lr2_y_hat_bin = LogisticRegression(penalty='l1', random_state=42)\n",
    "lr2_y_hat_proba = LogisticRegression(penalty='l1', random_state=42)\n",
    "\n",
    "lr2_y_hat_bin.fit(lr2_train_bin, y_train_original)\n",
    "lr2_y_hat_proba.fit(lr2_train_proba, y_train_original)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_pred_bin = lr2_y_hat_bin.predict(lr2_test_bin)\n",
    "y_test_pred_proba = lr2_y_hat_proba.predict(lr2_test_proba)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.84176\n",
      "0.84176\n",
      "\n",
      "0.89284\n",
      "0.89316\n"
     ]
    }
   ],
   "source": [
    "print(lr2_y_hat_bin.score(lr2_test_bin,y_test_original))\n",
    "print(lr2_y_hat_proba.score(lr2_test_proba,y_test_original))\n",
    "print()\n",
    "print(lr2_y_hat_bin.score(lr2_train_bin,y_train_original))\n",
    "print(lr2_y_hat_proba.score(lr2_train_proba,y_train_original))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1.51901016]\n",
      "[-1.6046603]\n"
     ]
    }
   ],
   "source": [
    "print(lr2_y_hat_bin.intercept_)\n",
    "print(lr2_y_hat_proba.intercept_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print_weight(lr2_y_hat_bin, lr2_y_hat_proba)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print_weight(lr2_y_hat_proba)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What if... we put scale and ReLU manually\n",
    "\n",
    "If we do not scale. Then, the negative weights would be closer to zero. The magnitude would be lesser."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_y = lr1.predict(X_train)\n",
    "proba_y = lr1.predict_proba(X_train)[:,1]\n",
    "\n",
    "# Scale\n",
    "\n",
    "predicted_y = predicted_y * 2 - 1\n",
    "proba_y = proba_y * 2 - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr2_train_bin = list()\n",
    "for i,y in enumerate(predicted_y):\n",
    "    if y == 1:\n",
    "        lr2_train_bin.append(y_train_agreement_pos[i] * y)\n",
    "    else:\n",
    "        lr2_train_bin.append(y_train_agreement_neg[i] * y)\n",
    "        \n",
    "lr2_train_bin = np.asarray(lr2_train_bin)\n",
    "\n",
    "lr2_train_proba = list()\n",
    "for i,y in enumerate(proba_y):\n",
    "    if y >= 0.5:\n",
    "        lr2_train_proba.append(y_train_agreement_pos[i] * y)\n",
    "    else:\n",
    "        lr2_train_proba.append(y_train_agreement_neg[i] * y)\n",
    "        \n",
    "lr2_train_proba = np.asarray(lr2_train_proba)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l1', random_state=42, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train with {0,1} label\n",
    "# Train with [0,1] proba\n",
    "\n",
    "lr2_y_hat_bin = LogisticRegression(penalty='l1', random_state=42)\n",
    "lr2_y_hat_proba = LogisticRegression(penalty='l1', random_state=42)\n",
    "\n",
    "lr2_y_hat_bin.fit(lr2_train_bin, y_train_original)\n",
    "lr2_y_hat_proba.fit(lr2_train_proba, y_train_original)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with predict {0,1}\n",
    "\n",
    "predicted_y_test = lr1.predict(X_test)\n",
    "predicted_y_test = predicted_y_test * 2 - 1\n",
    "\n",
    "lr2_test_bin = list()\n",
    "for i,y in enumerate(predicted_y_test):\n",
    "    if y==1:\n",
    "        lr2_test_bin.append(y_test_agreement_pos[i] * y)\n",
    "    else:\n",
    "        lr2_test_bin.append(y_test_agreement_neg[i] * y)\n",
    "    \n",
    "lr2_test_bin = np.asarray(lr2_test_bin)\n",
    "\n",
    "\n",
    "# Test with predict [0,1]\n",
    "\n",
    "predicted_y_test_proba = lr1.predict_proba(X_test)[:,1]\n",
    "predicted_y_test_proba = predicted_y_test_proba * 2 - 1\n",
    "\n",
    "lr2_test_proba = list()\n",
    "for i,y in enumerate(predicted_y_test_proba):\n",
    "    if y>=0:\n",
    "        lr2_test_proba.append(y_test_agreement_pos[i] * y)\n",
    "    else:\n",
    "        lr2_test_proba.append(y_test_agreement_neg[i] * y)\n",
    "    \n",
    "lr2_test_proba = np.asarray(lr2_test_proba)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.85604\n",
      "0.85648\n",
      "\n",
      "0.9054\n",
      "0.905\n"
     ]
    }
   ],
   "source": [
    "print(lr2_y_hat_bin.score(lr2_test_bin,y_test_original))\n",
    "print(lr2_y_hat_proba.score(lr2_test_proba,y_test_original))\n",
    "print()\n",
    "print(lr2_y_hat_bin.score(lr2_train_bin,y_train_original))\n",
    "print(lr2_y_hat_proba.score(lr2_train_proba,y_train_original))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "accept_indices = np.where(np.sum(lr2_test_proba,axis=1)!=0)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8966081658634173\n",
      "0.8971083022642539\n"
     ]
    }
   ],
   "source": [
    "print(lr2_y_hat_bin.score(lr2_test_bin[accept_indices], y_test_original[accept_indices]))\n",
    "print(lr2_y_hat_proba.score(lr2_test_proba[accept_indices], y_test_original[accept_indices]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3502273851498478"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import log_loss\n",
    "\n",
    "p = lr2_y_hat_proba.predict_proba(lr2_test_proba[accept_indices])[:,1]\n",
    "\n",
    "log_loss(y_test_original[accept_indices], p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([    8,    12,    36, ..., 24992, 24997, 24998], dtype=int64)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.where(np.sum(lr2_test_bin,axis=1)==0)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.12024"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(np.sum(lr2_test_proba,axis=1)==0)/lr2_test_proba.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3006"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(np.sum(lr2_test_proba,axis=1)==0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr2_test_proba[8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr2_test_bin[8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.31798309868414254"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_y_test_proba[8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_y_test[8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/10 \t 2.783 \t 2.919 \t 1\n",
      "7/10 \t 2.608 \t 2.802 \t 1\n",
      "noir \t 2.013 \t 3.067 \t 1\n",
      "superb \t 2.003 \t 3.073 \t 1\n",
      "brilliant \t 1.895 \t 2.570 \t 1\n",
      "10/10 \t 1.754 \t 1.916 \t 1\n",
      "9/10 \t 1.716 \t 1.455 \t 1\n",
      "refreshing \t 1.640 \t 1.462 \t 1\n",
      "incredible \t 1.631 \t 2.308 \t 1\n",
      "amazing \t 1.419 \t 2.182 \t 1\n",
      "excellent \t 1.410 \t 1.958 \t 1\n",
      "great \t 1.381 \t 2.359 \t 1\n",
      "fascinating \t 1.369 \t 2.367 \t 1\n",
      "beautiful \t 1.366 \t 2.417 \t 1\n",
      "enjoyed \t 1.346 \t 1.694 \t 1\n",
      "wonderful \t 1.326 \t 2.334 \t 1\n",
      "perfect \t 1.325 \t 2.366 \t 1\n",
      "surprisingly \t 1.309 \t 1.940 \t 1\n",
      "fantastic \t 1.296 \t 1.652 \t 1\n",
      "gem \t 1.294 \t 2.425 \t 1\n",
      "best \t 1.294 \t 2.351 \t 1\n",
      "favorite \t 1.290 \t 1.963 \t 1\n",
      "rare \t 1.252 \t 2.202 \t 1\n",
      "subtle \t 1.189 \t 2.220 \t 1\n",
      "solid \t 1.141 \t 1.982 \t 1\n",
      "fun \t 1.095 \t 2.272 \t 1\n",
      "recommended \t 1.070 \t 2.034 \t 1\n",
      "loved \t 1.023 \t 2.030 \t 1\n",
      "enjoyable \t 0.958 \t 1.987 \t 1\n",
      "beautifully \t 0.606 \t -0.172 \t 1\n",
      "wasted \t 0.509 \t 0.766 \t 0\n",
      "funniest \t 0.278 \t 0.068 \t 1\n",
      "5/10 \t 0.102 \t 0.432 \t 1\n",
      "wonderfully \t 0.029 \t 0.000 \t 1\n",
      "badly \t 0.000 \t 0.409 \t 0\n",
      "6/10 \t 0.000 \t 0.000 \t 1\n",
      "perfectly \t -0.068 \t -0.411 \t 1\n",
      "poorly \t -0.541 \t -0.719 \t 0\n",
      "bland \t -0.731 \t -1.503 \t 0\n",
      "sadly \t -0.789 \t -1.326 \t 0\n",
      "mst3k \t -0.823 \t -1.258 \t 0\n",
      "funny \t -0.830 \t -1.761 \t 0\n",
      "pathetic \t -0.840 \t -1.111 \t 0\n",
      "mediocre \t -0.846 \t -1.463 \t 0\n",
      "weak \t -0.943 \t -2.023 \t 0\n",
      "disappointed \t -0.981 \t -1.603 \t 0\n",
      "unfunny \t -1.021 \t -0.080 \t 0\n",
      "ridiculous \t -1.038 \t -1.618 \t 0\n",
      "unfortunately \t -1.065 \t -2.252 \t 0\n",
      "annoying \t -1.086 \t -1.623 \t 0\n",
      "terrible \t -1.158 \t -1.568 \t 0\n",
      "predictable \t -1.173 \t -2.050 \t 0\n",
      "forgettable \t -1.179 \t -1.804 \t 0\n",
      "cheap \t -1.189 \t -1.908 \t 0\n",
      "horrible \t -1.200 \t -1.752 \t 0\n",
      "dull \t -1.215 \t -1.641 \t 0\n",
      "stupid \t -1.253 \t -1.880 \t 0\n",
      "mess \t -1.286 \t -2.161 \t 0\n",
      "tedious \t -1.291 \t -1.542 \t 0\n",
      "disappointing \t -1.312 \t -1.954 \t 0\n",
      "poor \t -1.358 \t -1.929 \t 0\n",
      "lacks \t -1.400 \t -1.739 \t 0\n",
      "worse \t -1.405 \t -1.738 \t 0\n",
      "bad \t -1.418 \t -2.317 \t 0\n",
      "awful \t -1.448 \t -2.066 \t 0\n",
      "lame \t -1.467 \t -2.317 \t 0\n",
      "redeeming \t -1.476 \t -1.636 \t 0\n",
      "obnoxious \t -1.489 \t -2.323 \t 0\n",
      "boring \t -1.509 \t -2.432 \t 0\n",
      "laughable \t -1.544 \t -2.160 \t 0\n",
      "insult \t -1.594 \t -1.852 \t 0\n",
      "lousy \t -1.635 \t -2.132 \t 0\n",
      "avoid \t -1.677 \t -2.646 \t 0\n",
      "worst \t -1.680 \t -2.405 \t 0\n",
      "disappointment \t -1.741 \t -2.448 \t 0\n",
      "fails \t -1.750 \t -2.222 \t 0\n",
      "pointless \t -1.945 \t -2.035 \t 0\n",
      "dreadful \t -2.009 \t -1.999 \t 0\n",
      "waste \t -2.152 \t -2.779 \t 0\n",
      "4/10 \t -2.373 \t -2.471 \t 0\n",
      "1/10 \t -2.494 \t -2.344 \t 0\n",
      "2/10 \t -3.063 \t -2.794 \t 0\n",
      "3/10 \t -3.430 \t -3.157 \t 0\n"
     ]
    }
   ],
   "source": [
    "print_weight(lr2_y_hat_bin, lr2_y_hat_proba)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# becasue LR sklearn use a regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython import display\n",
    "\n",
    "from coloredweighteddoc import ColoredWeightedDoc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# review = list(['I was in Chicago last week. And I wanted to see a movie so bad. I am surprised that the movie itself is just amazing.  The plot was kinda weak, but it was great.  Christopher Nolan is just brilliant. Never fails to amaze. Even though the weather was terrible, but I enjoyed it. No regret! Overall, 9/10 . Recommended.'])\n",
    "\n",
    "review = list(['I went to the movie. The movie is terrible. Acting was great, but the plot was awful. The weather is excellent. 1/10 . Avoid it.'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l1', random_state=42, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr1 = LogisticRegression(C=1, random_state=42, penalty='l1')\n",
    "lr1.fit(X_train, y_train_original)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_cv = cv.transform(review)\n",
    "words = cv.get_feature_names()\n",
    "weights = lr1.coef_.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<font size = 1, color=gray> I </font><font size = 1, color=blue> went </font><font size = 1, color=red> to </font><font size = 1, color=blue> the </font><font size = 1, color=red> movie. </font><font size = 1, color=blue> The </font><font size = 1, color=red> movie </font><font size = 1, color=blue> is </font><font size = 1, color=red> terrible. </font><font size = 1, color=red> Acting </font><font size = 1, color=red> was </font><font size = 1, color=red> go, </font><font size = 1, color=blue> the </font><font size = 1, color=red> plot </font><font size = 1, color=red> was </font><font size = 2, color=red> awful. </font><font size = 1, color=blue> The </font><font size = 1, color=gray> weather </font><font size = 1, color=blue> is </font><font size = 2, color=blue> excellent. </font><font size = 1, color=red> 1/10 </font><font size = 1, color=gray> . </font><font size = 2, color=red> Avoid </font><font size = 1, color=blue> it. </font>"
      ],
      "text/plain": [
       "<coloredweighteddoc.ColoredWeightedDoc at 0x21199f3b550>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display.display(ColoredWeightedDoc(review[0], words, weights, binary = False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr1.predict(review_cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1/10',\n",
       " '2/10',\n",
       " '3/10',\n",
       " '4/10',\n",
       " '5/10',\n",
       " '6/10',\n",
       " '7/10',\n",
       " '8/10',\n",
       " '9/10',\n",
       " '10/10',\n",
       " 'amazing',\n",
       " 'annoying',\n",
       " 'avoid',\n",
       " 'awful',\n",
       " 'bad',\n",
       " 'badly',\n",
       " 'beautiful',\n",
       " 'beautifully',\n",
       " 'best',\n",
       " 'bland',\n",
       " 'boring',\n",
       " 'brilliant',\n",
       " 'cheap',\n",
       " 'disappointed',\n",
       " 'disappointing',\n",
       " 'disappointment',\n",
       " 'dreadful',\n",
       " 'dull',\n",
       " 'enjoyable',\n",
       " 'enjoyed',\n",
       " 'excellent',\n",
       " 'fails',\n",
       " 'fantastic',\n",
       " 'fascinating',\n",
       " 'favorite',\n",
       " 'forgettable',\n",
       " 'fun',\n",
       " 'funny',\n",
       " 'funniest',\n",
       " 'gem',\n",
       " 'great',\n",
       " 'horrible',\n",
       " 'incredible',\n",
       " 'insult',\n",
       " 'lacks',\n",
       " 'lame',\n",
       " 'laughable',\n",
       " 'lousy',\n",
       " 'loved',\n",
       " 'mediocre',\n",
       " 'mess',\n",
       " 'mst3k',\n",
       " 'noir',\n",
       " 'obnoxious',\n",
       " 'pathetic',\n",
       " 'perfect',\n",
       " 'perfectly',\n",
       " 'pointless',\n",
       " 'poor',\n",
       " 'poorly',\n",
       " 'predictable',\n",
       " 'rare',\n",
       " 'recommended',\n",
       " 'redeeming',\n",
       " 'refreshing',\n",
       " 'ridiculous',\n",
       " 'sadly',\n",
       " 'solid',\n",
       " 'stupid',\n",
       " 'subtle',\n",
       " 'superb',\n",
       " 'surprisingly',\n",
       " 'tedious',\n",
       " 'terrible',\n",
       " 'unfortunately',\n",
       " 'unfunny',\n",
       " 'waste',\n",
       " 'wasted',\n",
       " 'weak',\n",
       " 'wonderful',\n",
       " 'wonderfully',\n",
       " 'worse',\n",
       " 'worst']"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.00044398])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr1.predict_proba(review_cv)[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/10 -4.980492132096975\n",
      "1/10 -4.937107261864974\n",
      "4/10 -4.824637583352251\n",
      "2/10 -4.6176845612231165\n",
      "unwatchable -3.271299911592406\n",
      "incoherent -2.250700821801265\n",
      "unfunny -2.2279709510711276\n",
      "stinker -2.1861393041620403\n",
      "waste -2.129423651160658\n",
      "disappointment -2.0909189158850356\n",
      "poorly -2.0801273102448348\n",
      "behave -2.0368013490575243\n",
      "worst -1.9320372991334245\n",
      "miscast -1.850236734715928\n",
      "uninspired -1.8247514115870782\n",
      "appalling -1.7576297833333383\n",
      "pointless -1.7575713622081954\n",
      "lousy -1.735310024667366\n",
      "cardboard -1.7300221393521815\n",
      "boredom -1.7241510728677027\n",
      "forgettable -1.7140029410285202\n",
      "mildly -1.6789299445531114\n",
      "obnoxious -1.678676401827334\n",
      "awful -1.6780891624228707\n",
      "mst3k -1.6649308372763307\n",
      "fails -1.6167210166756092\n",
      "fest -1.615303586857381\n",
      "wooden -1.603304904784953\n",
      "furthermore -1.5881262252686443\n",
      "laughable -1.5728643342134705\n"
     ]
    }
   ],
   "source": [
    "indices = np.argsort(weights)\n",
    "\n",
    "for i in indices[:30]:\n",
    "    print(words[i], weights[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "ht_lr = LogisticRegression(penalty='l1', random_state=42)\n",
    "\n",
    "cv_ht = CountVectorizer(binary=True, vocabulary=word_list)\n",
    "X = cv_ht.fit_transform(X_train_original)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l1', random_state=42, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ht_lr.fit(X, y_train_original)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "ht_ex = cv_ht.transform(review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ht_lr.predict(ht_ex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = ht_lr.coef_.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<font size = 1, color=gray> I </font><font size = 1, color=gray> went </font><font size = 1, color=gray> to </font><font size = 1, color=gray> the </font><font size = 1, color=gray> movie. </font><font size = 1, color=gray> The </font><font size = 1, color=gray> movie </font><font size = 1, color=gray> is </font><font size = 4, color=red> terrible. </font><font size = 1, color=gray> Acting </font><font size = 1, color=gray> was </font><font size = 1, color=gray> go, </font><font size = 1, color=gray> the </font><font size = 1, color=gray> plot </font><font size = 1, color=gray> was </font><font size = 6, color=red> awful. </font><font size = 1, color=gray> The </font><font size = 1, color=gray> weather </font><font size = 1, color=gray> is </font><font size = 5, color=blue> excellent. </font><font size = 1, color=gray> 1/10 </font><font size = 1, color=gray> . </font><font size = 4, color=red> Avoid </font><font size = 1, color=gray> it. </font>"
      ],
      "text/plain": [
       "<coloredweighteddoc.ColoredWeightedDoc at 0x21199f3bf98>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display.display(ColoredWeightedDoc(review[0], word_list, w, binary = False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
