{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy.random import seed\n",
    "seed(42)\n",
    "# load data\n",
    "# make sure that the first shape is the IMDB training data. \n",
    "\n",
    "def open_pickle(path):\n",
    "    import pickle\n",
    "    with open(path, 'rb') as f:\n",
    "        X = pickle.load(f)\n",
    "    return X\n",
    "\n",
    "def load_unigrams(path, X, y):\n",
    "    word_list = []\n",
    "    connotation = {}\n",
    "    \n",
    "    with open(path, 'r', encoding='utf8') as f:\n",
    "        for line in f:\n",
    "            word_list.append(line.strip())\n",
    "            \n",
    "    for word in word_list:\n",
    "        pos_count = 0\n",
    "        neg_count = 0\n",
    "        for i, doc in enumerate(X):\n",
    "            if word in doc.lower():\n",
    "                \n",
    "                if (y[i] == 1):\n",
    "                    pos_count += 1\n",
    "                else:\n",
    "                    neg_count += 1\n",
    "                    \n",
    "        if pos_count > neg_count:\n",
    "            connotation[word] = 1\n",
    "        else:\n",
    "            connotation[word] = 0\n",
    "    \n",
    "    return word_list, connotation\n",
    "\n",
    "def generate_appearance(X_train_corpus, X_test_corpus, word_list, connotation):\n",
    "    y_train_agreement = []\n",
    "    for i in range(len(X_train_corpus)):\n",
    "        doc_agreement = []\n",
    "        for word in word_list:\n",
    "            if word in X_train_corpus[i]:\n",
    "                if connotation[word] == 1:\n",
    "                    doc_agreement.append(1)\n",
    "                else:\n",
    "                    doc_agreement.append(-1)\n",
    "            else:\n",
    "                doc_agreement.append(0)\n",
    "        y_train_agreement.append(doc_agreement)\n",
    "        \n",
    "    y_test_agreement = []\n",
    "    for i in range(len(X_test_corpus)):\n",
    "        doc_agreement = []\n",
    "        for word in word_list:\n",
    "            if word in X_test_corpus[i]:\n",
    "                if connotation[word] == 1:\n",
    "                    doc_agreement.append(1)\n",
    "                else:\n",
    "                    doc_agreement.append(-1)\n",
    "            else:\n",
    "                doc_agreement.append(0)\n",
    "        y_test_agreement.append(doc_agreement)\n",
    "        \n",
    "    return np.array(y_train_agreement), np.array(y_test_agreement)\n",
    "\n",
    "# 'imdb-unigrams.txt'\n",
    "\n",
    "X_train_original = open_pickle('../../data/imdb/imdb_original_preprocessed_xtrain.pickle')\n",
    "X_test_original = open_pickle('../../data/imdb/imdb_original_preprocessed_xtest.pickle')\n",
    "y_train_original = open_pickle('../../data/imdb/imdb_original_preprocessed_ytrain.pickle')\n",
    "y_test_original = open_pickle('../../data/imdb/imdb_original_preprocessed_ytest.pickle')\n",
    "\n",
    "# Count vectorizer \n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "token = r\"(?u)\\b[\\w\\'/]+\\b\"\n",
    "cv = CountVectorizer(min_df = 100, token_pattern=token, lowercase=True, binary=True)\n",
    "X_train = cv.fit_transform(X_train_original)\n",
    "X_test = cv.transform(X_test_original)\n",
    "\n",
    "word_list, connotation = load_unigrams('./imdb-unigrams.txt', X_train_original, y_train_original)\n",
    "\n",
    "y_train_agreement, y_test_agreement = generate_appearance(X_train_original, X_test_original, \n",
    "                                                          word_list, connotation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow import set_random_seed\n",
    "seed(42)\n",
    "set_random_seed(42)\n",
    "\n",
    "from keras.layers import Input, Dense, TimeDistributed, Embedding\n",
    "from keras.layers import Concatenate, Reshape, Lambda, Multiply, multiply, concatenate\n",
    "from keras.models import Model\n",
    "from keras import backend as K\n",
    "\n",
    "\n",
    "\n",
    "def accuracy_reject(combined, X, y_agreement, y):\n",
    "    \n",
    "    # define model which get the input from combined model\n",
    "    # output the value after relu\n",
    "    human_terms_relu_model = Model(inputs=combined.input,\n",
    "                                    outputs=combined.get_layer('concatenate').output)\n",
    "    predict_relu = human_terms_relu_model.predict([X, y_agreement])\n",
    "    accept_indices = np.where(np.sum(predict_relu, axis=1)!=0)\n",
    "    accept_indices = accept_indices[0]\n",
    "    total_reject = X.shape[0] - len(accept_indices)\n",
    "    rejection_rate = total_reject/X.shape[0]\n",
    "\n",
    "    test_eval = combined.evaluate([X[accept_indices], y_agreement[accept_indices]], y[accept_indices])\n",
    "    \n",
    "    return test_eval, rejection_rate\n",
    "\n",
    "# make first model\n",
    "\n",
    "def build_base_model(input_shape):\n",
    "    input_layer = Input(shape=(input_shape,))\n",
    "#     tanh_output = Dense(1, activation='tanh', name='tanh_output')(input_layer)\n",
    "    tanh_output = Dense(1, activation='sigmoid')(input_layer)\n",
    "    \n",
    "    model = Model(inputs=input_layer, outputs=tanh_output)\n",
    "    model.summary()\n",
    "    return model\n",
    "\n",
    "\n",
    "def layer_split(x):\n",
    "    return tf.split(x,num_or_size_splits=human_terms_len,axis=1)\n",
    "\n",
    "def layer_concat(x):\n",
    "    return tf.concat(x, axis=1)\n",
    "\n",
    "# build the combined model\n",
    "# Combined model\n",
    "human_terms_len = len(word_list)\n",
    "\n",
    "base_model = build_base_model(X_train.shape[1])\n",
    "\n",
    "combined_input_layer = Input(shape=(X_train.shape[1],))\n",
    "\n",
    "# build the hard coded weight for human terms\n",
    "ht_input_layer = Input(shape=(human_terms_len,))\n",
    "\n",
    "# split = Lambda( lambda x: tf.split(x,num_or_size_splits=human_terms_len,axis=1))(ht_input_layer)\n",
    "split = Lambda(layer_split)(ht_input_layer)\n",
    "\n",
    "\n",
    "# get the document prediction\n",
    "label_layer = base_model(combined_input_layer)\n",
    "\n",
    "# do normalize of bipolar sigmoid\n",
    "\n",
    "\n",
    "# stack the multiply layer\n",
    "dense_layer = []\n",
    "for i in range(human_terms_len):\n",
    "    dense_layer.append(Dense(1, activation='relu',use_bias=False, kernel_initializer='ones')(Multiply()([split[i], label_layer])))\n",
    "\n",
    "# concat all the result   \n",
    "# concat = Lambda( lambda x: tf.concat(x, axis=1), name='concatenate')(dense_layer)\n",
    "concat = Lambda(layer_concat, name='concatenate')(dense_layer)\n",
    "\n",
    "\n",
    "# pass it to sigmoid layer\n",
    "output_layer = Dense(1, activation='sigmoid')(concat)\n",
    "\n",
    "combined_model = Model(inputs=[combined_input_layer, ht_input_layer], outputs=output_layer)\n",
    "# combined_model.summary()\n",
    "\n",
    "base_model.compile(loss='binary_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['acc'])\n",
    "\n",
    "# combined_model.compile(loss='mse',\n",
    "#                       optimizer='adam',\n",
    "#                       metrics=['acc'])\n",
    "\n",
    "y_train_tanh = y_train_original\n",
    "y_train_tanh[y_train_tanh == 0] = -1\n",
    "\n",
    "y_test_tanh = y_test_original\n",
    "y_test_tanh[y_test_tanh == 0] = -1\n",
    "\n",
    "base_model_history = base_model.fit(X_train[:16667], y_train_original[:16667], \n",
    "                                    validation_data=(X_train[16667:], y_train_original[16667:]),\n",
    "                                    batch_size=1, epochs=1)\n",
    "\n",
    "# combined_model_history = combined_model.fit([X_train[:16667],y_train_agreement[:16667]], y_train_original[:16667], \n",
    "#                                             validation_data=([X_train[16667:], y_train_agreement[16667:]], y_train_original[16667:]),\n",
    "#                                             batch_size=1, epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_accuracy(y_true, y_pred):\n",
    "    return K.mean(K.equal(y_true, K.round(y_pred)), axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = combined_model.evaluate([X_test, y_test_agreement], y_test_original)\n",
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21806/21806 [==============================] - 2s 82us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([0.6911189293496206, 0.3853985141717785], 0.12776)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_reject(combined_model, X_test, y_test_agreement, y_test_original)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000/25000 [==============================] - 2s 65us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.6867102897262574, 0.35676]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score = combined_model.evaluate([X_train, y_train_agreement], y_train_original)\n",
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21864/21864 [==============================] - 1s 62us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([0.6436229766920029, 0.4079308452250274], 0.12544)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_reject(combined_model, X_train, y_train_agreement, y_train_original)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "clf = LogisticRegression(random_state=42)\n",
    "\n",
    "clf.fit(X_train, y_train_original)\n",
    "print(clf.score(X_test, y_test_original))\n",
    "print(clf.score(X_train, y_train_original))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print report on the word transparency\n",
    "index = [9, 19]\n",
    "def report():\n",
    "    for i in index:\n",
    "        print()\n",
    "        bm = base_model.predict(X_test[i])\n",
    "        \n",
    "        cm = combined_model.predict([X_test[i], \n",
    "                                np.reshape(y_test_agreement[i], (1,y_test_agreement.shape[1]))])\n",
    "        \n",
    "#         document_output = 'multiply'\n",
    "#         document_predict = Model(inputs=combined_model.input,\n",
    "#                                      outputs=combined_model.get_layer(document_output).output)\n",
    "#         doc_output = document_predict.predict([np.reshape(data[i], (1,5)), \n",
    "#                                       ht_1_input[i], \n",
    "#                                       ht_2_input[i], \n",
    "#                                       ht_3_input[i], \n",
    "#                                       ht_4_input[i]])\n",
    "        \n",
    "        layer_name = 'concatenate'\n",
    "        concat_after_relu = Model(inputs=combined_model.input,\n",
    "                                     outputs=combined_model.get_layer(layer_name).output)\n",
    "        concat_output = concat_after_relu.predict([X_test[i], \n",
    "                                np.reshape(y_test_agreement[i], (1,y_test_agreement.shape[1]))])\n",
    "        \n",
    "        print(X_test_original[i], '\\n\\n actual label : ', y_test_original[i], '\\n predict from base model : ', bm.flatten(), '\\n predict label : ', cm.flatten())\n",
    "    \n",
    "        for i,output in enumerate(concat_output.flatten()):\n",
    "            if output != 0:\n",
    "                print(word_list[i], output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = combined_model.evaluate([X_test, y_test_agreement], y_test_original)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's see the weight when the trainable is false"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_combined_model():\n",
    "    # build the combined model\n",
    "    # Combined model\n",
    "    human_terms_len = len(word_list)\n",
    "\n",
    "    base_model = build_base_model(X_train.shape[1])\n",
    "\n",
    "    combined_input_layer = Input(shape=(X_train.shape[1],))\n",
    "\n",
    "    # build the hard coded weight for human terms\n",
    "    ht_input_layer = Input(shape=(human_terms_len,))\n",
    "\n",
    "    split = Lambda( lambda x: tf.split(x,num_or_size_splits=human_terms_len,axis=1))(ht_input_layer)\n",
    "\n",
    "    # get the document prediction\n",
    "    label_layer = base_model(combined_input_layer)\n",
    "\n",
    "    # stack the multiply layer\n",
    "    dense_layer = []\n",
    "    for i in range(human_terms_len):\n",
    "        dense_layer.append(Dense(1, activation='relu',use_bias=False, kernel_initializer='ones', trainable=True)(Multiply()([split[i], label_layer])))\n",
    "\n",
    "    # concat all the result   \n",
    "    concat = Lambda( lambda x: tf.concat(x, axis=1), name='concatenate')(dense_layer)\n",
    "\n",
    "    # pass it to sigmoid layer\n",
    "    output_layer = Dense(1, activation='sigmoid')(concat)\n",
    "\n",
    "    combined_model = Model(inputs=[combined_input_layer, ht_input_layer], outputs=output_layer)\n",
    "    combined_model.summary()\n",
    "    \n",
    "    return base_model, combined_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "false_base_model, false_combined_model = build_combined_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "false_base_model.compile(loss='mse',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['acc'])\n",
    "\n",
    "false_base_model.trainable=False\n",
    "\n",
    "false_combined_model.compile(loss='mse',\n",
    "                      optimizer='adam',\n",
    "                      metrics=['acc'])\n",
    "\n",
    "base_model_history_train_false = false_base_model.fit(X_train[:16667], y_train_tanh[:16667], \n",
    "                                    validation_data=(X_train[16667:], y_train_tanh[16667:]),\n",
    "                                    batch_size=1, epochs=1)\n",
    "\n",
    "combined_model_history_train_false = false_combined_model.fit([X_train[:16667],y_train_agreement[:16667]], y_train_original[:16667], batch_size=1, epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate model\n",
    "\n",
    "score = false_combined_model.evaluate([X_test, y_test_agreement], y_test_original)\n",
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_reject(false_combined_model, X_test, y_test_agreement, y_test_original)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = false_combined_model.evaluate([X_train, y_train_agreement], y_train_original)\n",
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_reject(false_combined_model, X_train, y_train_agreement, y_train_original)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
