{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Note\n",
    "\n",
    "#### 1. use kernel_initializer = 'ones' instead of 'zeros' -> the gradients do not flow through the ReLu layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dummy files\n",
    "np.random.seed(42)\n",
    "EMBEDDING_SIZE = 15\n",
    "X = np.random.randint(2, size=(1000,100))\n",
    "X_val = np.random.randint(2, size=(1000,100))\n",
    "y = np.random.randint(2, size=1000)\n",
    "y_val = np.random.randint(2, size=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 100)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000,)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Anneke\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Input, Dense, TimeDistributed, Embedding\n",
    "from keras.layers import Concatenate, Reshape, Lambda, Multiply, multiply, concatenate, ThresholdedReLU\n",
    "from keras.models import Model\n",
    "from keras import backend as K\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_tanh = []\n",
    "y_tanh_val = []\n",
    "\n",
    "for i in y:\n",
    "    if i==0:\n",
    "        y_tanh.append(-1)\n",
    "    else:\n",
    "        y_tanh.append(1)\n",
    "\n",
    "\n",
    "for i in y_val:\n",
    "    if i==0:\n",
    "        y_tanh_val.append(-1)\n",
    "    else:\n",
    "        y_tanh_val.append(1)\n",
    "        \n",
    "y_tanh = np.asarray(y_tanh)  \n",
    "y_tanh_val = np.asarray(y_tanh_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "tanh_output (Dense)          (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 101\n",
      "Trainable params: 101\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# make first model\n",
    "\n",
    "def build_base_model(input_shape):\n",
    "    input_layer = Input(shape=(input_shape,))\n",
    "    tanh_output = Dense(1, activation='tanh', name='tanh_output')(input_layer)\n",
    "    \n",
    "    model = Model(inputs=input_layer, outputs=tanh_output)\n",
    "    model.summary()\n",
    "    return model\n",
    "\n",
    "base_model = build_base_model(X.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            (None, 100)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_3 (InputLayer)            (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "model_1 (Model)                 (None, 1)            101         input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "input_4 (InputLayer)            (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "multiply_1 (Multiply)           (None, 1)            0           input_3[0][0]                    \n",
      "                                                                 model_1[1][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "multiply_2 (Multiply)           (None, 1)            0           input_4[0][0]                    \n",
      "                                                                 model_1[1][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 1)            2           multiply_1[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 1)            2           multiply_2[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 2)            0           dense_1[0][0]                    \n",
      "                                                                 dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 1)            3           concatenate_1[0][0]              \n",
      "==================================================================================================\n",
      "Total params: 108\n",
      "Trainable params: 108\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Combined model\n",
    "\n",
    "combined_input_layer = Input(shape=(X.shape[1],))\n",
    "\n",
    "ht_1 = Input(shape=(1,))\n",
    "ht_2 = Input(shape=(1,))\n",
    "\n",
    "label_layer = base_model(combined_input_layer)\n",
    "\n",
    "dense_1 = Dense(1, activation='relu')(Multiply()([ht_1, label_layer]))\n",
    "dense_2 = Dense(1, activation='relu')(Multiply()([ht_2, label_layer]))\n",
    "\n",
    "concat = Concatenate()([dense_1, dense_2])\n",
    "\n",
    "output_layer = Dense(1, activation='sigmoid')(concat)\n",
    "\n",
    "combined_model = Model(inputs=[combined_input_layer, ht_1, ht_2], outputs=output_layer)\n",
    "combined_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model.compile(loss='mse',\n",
    "            optimizer='adam',\n",
    "            metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_model.compile(loss='mse',\n",
    "                      optimizer='adam',\n",
    "                      metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make second model\n",
    "# first, make the vector to see if it is appears \n",
    "\n",
    "# we have two terms (human-terms) for now. c1 == pos word, c2 == neg word\n",
    "# 1st column gives the feature index where that particular human_terms located\n",
    "# 2nd column gives whether it is positive/negative word\n",
    "term = [[1,1],[2,-1]]\n",
    "term = np.asarray(term)\n",
    "\n",
    "#assume X is a document vector [1,n] where n is the number of terms\n",
    "\n",
    "def np_present_by_term(X, term):\n",
    "    np_vector = np.zeros([term.shape[0],1])\n",
    "    \n",
    "    for i,(feature_index,np_term) in enumerate(term):\n",
    "        if X[feature_index] == 1:\n",
    "            np_vector[i] = np_term\n",
    "    \n",
    "    return np_vector\n",
    "\n",
    "# Assuming 'term' is a [1X2] vector\n",
    "def np_present_by_doc(X, term):\n",
    "    np_vector = np.zeros([X.shape[0],1])\n",
    "    \n",
    "    for i, x in enumerate(X):\n",
    "        if x[term[0]] == 1:\n",
    "            np_vector[i] = term[1]\n",
    "    \n",
    "    return np_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "ht_1_input = np_present_by_doc(X, term[0])\n",
    "ht_2_input = np_present_by_doc(X, term[1])\n",
    "\n",
    "ht_1_input_val = np_present_by_doc(X_val, term[0])\n",
    "ht_2_input_val = np_present_by_doc(X_val, term[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1000 samples, validate on 1000 samples\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "ename": "InternalError",
     "evalue": "Blas GEMV launch failed:  m=100, n=32\n\t [[Node: tanh_output/MatMul = MatMul[T=DT_FLOAT, _class=[\"loc:@training/Adam/gradients/tanh_output/MatMul_grad/MatMul_1\"], transpose_a=false, transpose_b=false, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](_arg_input_1_0_0/_53, tanh_output/kernel/read)]]\n\t [[Node: metrics/acc/Mean_1/_75 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_262_metrics/acc/Mean_1\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInternalError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-3ae1f20cc6f5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mbase_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_tanh\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_val\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_tanh_val\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[0;32m   1040\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1041\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1042\u001b[1;33m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[0;32m   1043\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1044\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[1;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[0;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    198\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 199\u001b[1;33m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    200\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    201\u001b[0m                     \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2659\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2660\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2661\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2662\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2663\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2629\u001b[0m                                 \u001b[0msymbol_vals\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2630\u001b[0m                                 session)\n\u001b[1;32m-> 2631\u001b[1;33m         \u001b[0mfetched\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2632\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2633\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1452\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1453\u001b[0m           return tf_session.TF_DeprecatedSessionRunCallable(\n\u001b[1;32m-> 1454\u001b[1;33m               self._session._session, self._handle, args, status, None)\n\u001b[0m\u001b[0;32m   1455\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1456\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__del__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\errors_impl.py\u001b[0m in \u001b[0;36m__exit__\u001b[1;34m(self, type_arg, value_arg, traceback_arg)\u001b[0m\n\u001b[0;32m    517\u001b[0m             \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    518\u001b[0m             \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mc_api\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_Message\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 519\u001b[1;33m             c_api.TF_GetCode(self.status.status))\n\u001b[0m\u001b[0;32m    520\u001b[0m     \u001b[1;31m# Delete the underlying status object from memory otherwise it stays alive\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    521\u001b[0m     \u001b[1;31m# as there is a reference to status from this from the traceback due to\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mInternalError\u001b[0m: Blas GEMV launch failed:  m=100, n=32\n\t [[Node: tanh_output/MatMul = MatMul[T=DT_FLOAT, _class=[\"loc:@training/Adam/gradients/tanh_output/MatMul_grad/MatMul_1\"], transpose_a=false, transpose_b=false, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](_arg_input_1_0_0/_53, tanh_output/kernel/read)]]\n\t [[Node: metrics/acc/Mean_1/_75 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_262_metrics/acc/Mean_1\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]"
     ]
    }
   ],
   "source": [
    "base_model.fit(X, y_tanh, validation_data=(X_val, y_tanh_val), epochs=10, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_model.fit([X, ht_1_input, ht_2_input], y, \n",
    "                   validation_data=([X_val, ht_1_input_val, ht_2_input_val], y_val), \n",
    "                   epochs=10, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple Domain Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate 16 documents\n",
    "# n1 = negative word with importance 1\n",
    "# p2 = positive word with importance 2\n",
    "# p3 = positive word with importance 3\n",
    "# n4 = negative word with importance 4\n",
    "# movie = neutral word with importance 0\n",
    "\n",
    "word_list = ['movie', 'bad', 'good', 'amazing', 'awful']\n",
    "word_weight = np.asarray([0, -1, 2, 3, -4], dtype='int32')\n",
    "word_weight.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import combinations\n",
    "\n",
    "num = 16\n",
    "data = np.zeros([num, len(word_list)])\n",
    "index=0\n",
    "label = []\n",
    "\n",
    "for i in range(5):\n",
    "    # get the combination\n",
    "    perm = combinations([1,2,3,4], i)\n",
    "    \n",
    "    for j in list(perm):\n",
    "        # always put neutral word\n",
    "        data[index, 0] = 1\n",
    "        \n",
    "        # if its only 1 terms\n",
    "        if i==1:\n",
    "            data[index,j] = 1\n",
    "        else:\n",
    "            for k in j:\n",
    "                data[index, k] = 1\n",
    "        \n",
    "        # calculate weights\n",
    "        \n",
    "        weights = data[index,:] * word_weight\n",
    "        label.append(np.sign(np.sum(weights)))\n",
    "        \n",
    "        index+=1\n",
    "        \n",
    "label = np.asarray(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np_label = (label == 1).astype('int32')\n",
    "np_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build the combined model\n",
    "# Combined model\n",
    "###########################################\n",
    "human_terms_len = 4\n",
    "thresholded_relu = ThresholdedReLU(theta=0.9)\n",
    "###########################################\n",
    "\n",
    "base_model = build_base_model(data.shape[1])\n",
    "\n",
    "combined_input_layer = Input(shape=(data.shape[1],))\n",
    "\n",
    "# build the hard coded weight for human terms\n",
    "ht_input_layer = Input(shape=(human_terms_len,))\n",
    "\n",
    "split = Lambda( lambda x: tf.split(x,num_or_size_splits=human_terms_len,axis=1))(ht_input_layer)\n",
    "\n",
    "# get the document prediction\n",
    "label_layer = base_model(combined_input_layer)\n",
    "\n",
    "# multiply and pass it into relu\n",
    "# initialize relu layer\n",
    "\n",
    "# stack the multiply layer\n",
    "dense_layer = []\n",
    "for i in range(human_terms_len):\n",
    "    dense_layer.append(Dense(1, \n",
    "                             activation='relu', \n",
    "                             use_bias=False, \n",
    "                             kernel_initializer='ones')(Multiply()([split[i], label_layer])))\n",
    "\n",
    "# concat all the result   \n",
    "concat = Lambda( lambda x: tf.concat(x, axis=1), name='concatenate')(dense_layer)\n",
    "\n",
    "# pass it to sigmoid layer\n",
    "output_layer = Dense(1, activation='sigmoid', use_bias=False)(concat)\n",
    "\n",
    "combined_model = Model(inputs=[combined_input_layer, ht_input_layer], outputs=output_layer)\n",
    "combined_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model.compile(loss='mse',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['acc'])\n",
    "\n",
    "combined_model.compile(loss='mse',\n",
    "                      optimizer='adam',\n",
    "                      metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def np_present_by_doc(X, index, weight):\n",
    "    np_vector = np.zeros([X.shape[0],1])\n",
    "    \n",
    "    for i, x in enumerate(X):\n",
    "        if x[index] == 1:\n",
    "            np_vector[i] = np.sign(weight)\n",
    "#             np_vector[i] = 1\n",
    "    \n",
    "    return np_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I need to generate the ht's\n",
    "\n",
    "ht_1_input = np_present_by_doc(data, 1, word_weight[1])\n",
    "ht_2_input = np_present_by_doc(data, 2, word_weight[2])\n",
    "ht_3_input = np_present_by_doc(data, 3, word_weight[3])\n",
    "ht_4_input = np_present_by_doc(data, 4, word_weight[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ht_1_input.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ht_input = np.hstack([ht_1_input, ht_2_input, ht_3_input, ht_4_input])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ht_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(ht_1_input)):\n",
    "    print(data[i], ht_4_input[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # adjust y to -> [-1, 1]\n",
    "# label_tanh = []\n",
    "# for i in label:\n",
    "#     if i==0:\n",
    "#         label_tanh.append(-1)\n",
    "#     else:\n",
    "#         label_tanh.append(1)\n",
    "# label_tanh = np.asarray(label_tanh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model.fit(data[1:15], label[1:15], epochs=1000, batch_size=1, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = base_model.predict(data[1:15])\n",
    "for i in range(len(y)):\n",
    "    print(data[i+1], label[i+1], y[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_model.fit([data[1:15], ht_input[1:15]], \n",
    "                   np_label[1:15], \n",
    "                   epochs=500,\n",
    "                   batch_size=1,\n",
    "                   verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net_weight = combined_model.get_weights()\n",
    "net_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('word_sign \\t Tanh_w \\t ReLu_w \\t sigmoid_w \\t word_importance')\n",
    "\n",
    "for i in range(4):\n",
    "    print(np.sign(word_weight[i+1]), '\\t', net_weight[0].flatten()[i+1], \n",
    "          '\\t', net_weight[i+2].flatten(), '\\t',net_weight[6].flatten()[i], '\\t',\n",
    "         net_weight[i+2].flatten()*net_weight[6].flatten()[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def report():\n",
    "    for i in range(1,15):\n",
    "        bm = base_model.predict(np.reshape(data[i], (1,5)))\n",
    "        \n",
    "        cm = combined_model.predict([np.reshape(data[i], (1,5)), \n",
    "                                np.reshape(ht_input[i], (1,4))])\n",
    "        \n",
    "#         document_output = 'multiply'\n",
    "#         document_predict = Model(inputs=combined_model.input,\n",
    "#                                      outputs=combined_model.get_layer(document_output).output)\n",
    "#         doc_output = document_predict.predict([np.reshape(data[i], (1,5)), \n",
    "#                                       ht_1_input[i], \n",
    "#                                       ht_2_input[i], \n",
    "#                                       ht_3_input[i], \n",
    "#                                       ht_4_input[i]])\n",
    "        \n",
    "        layer_name = 'concatenate'\n",
    "        concat_after_relu = Model(inputs=combined_model.input,\n",
    "                                     outputs=combined_model.get_layer(layer_name).output)\n",
    "        concat_output = concat_after_relu.predict([np.reshape(data[i], (1,5)), \n",
    "                                     np.reshape(ht_input[i], (1,4))])\n",
    "        \n",
    "        print(data[i], '\\t', np_label[i], '\\t', bm.flatten(), '\\t', cm.flatten(), '\\t',concat_output.flatten())\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ht_input.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report()\n",
    "# need to modify the ht_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "\n",
    "# base_model.save('overfit_base_model.h5')\n",
    "# combined_model.save('overfit_combined_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bm = base_model.predict(np.reshape(data[test_idx], (1,5)))\n",
    "# print(bm)\n",
    "\n",
    "# before jointly trained.\n",
    "# 0.7995629"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IMDB network Model\n",
    "#### concat and split layer test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build the combined model\n",
    "# Combined model\n",
    "human_terms_len = 4\n",
    "\n",
    "base_model = build_base_model(data.shape[1])\n",
    "\n",
    "combined_input_layer = Input(shape=(data.shape[1],))\n",
    "\n",
    "# build the hard coded weight for human terms\n",
    "ht_input_layer = Input(shape=(human_terms_len,))\n",
    "\n",
    "split = Lambda( lambda x: tf.split(x,num_or_size_splits=human_terms_len,axis=1))(ht_input_layer)\n",
    "\n",
    "# get the document prediction\n",
    "label_layer = base_model(combined_input_layer)\n",
    "\n",
    "# multiply and pass it into relu\n",
    "# initialize relu layer\n",
    "\n",
    "relu_layer = Dense(1, activation='relu', name='relu_layer', use_bias=False, kernel_initializer='ones')\n",
    "\n",
    "# stack the multiply layer\n",
    "dense_layer = []\n",
    "for i in range(human_terms_len):\n",
    "    dense_layer.append(relu_layer(Multiply()([split[i], label_layer])))\n",
    "\n",
    "# concat all the result   \n",
    "concat = Lambda( lambda x: tf.concat(x, axis=1), name='concatenate')(dense_layer)\n",
    "\n",
    "# pass it to sigmoid layer\n",
    "output_layer = Dense(1, activation='sigmoid')(concat)\n",
    "\n",
    "combined_model = Model(inputs=[combined_input_layer, ht_input_layer], outputs=output_layer)\n",
    "combined_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model.compile(loss='mse',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['acc'])\n",
    "\n",
    "combined_model.compile(loss='mse',\n",
    "                      optimizer='adam',\n",
    "                      metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model.fit(data[1:15], label[1:15], epochs=1000, batch_size=1, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_model.fit([data[1:15], ht_input[1:15]], \n",
    "                   np_label[1:15], \n",
    "                   epochs=500,\n",
    "                   batch_size=1,\n",
    "                   verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def np_present_by_doc(X, human_terms, index, weight):\n",
    "    np_vector = np.zeros([X.shape[0],])\n",
    "    \n",
    "    for i, x in enumerate(X):\n",
    "        if x[index] == 1:\n",
    "            np_vector[i] = np.sign(weight)\n",
    "    \n",
    "    return np_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_unigrams(path, X, y):\n",
    "    word_list = []\n",
    "    connotation = {}\n",
    "    \n",
    "    with open(path, 'r', encoding='utf8') as f:\n",
    "        for line in f:\n",
    "            word_list.append(line.strip())\n",
    "            \n",
    "    for word in word_list:\n",
    "        pos_count = 0\n",
    "        neg_count = 0\n",
    "        for i, doc in enumerate(X):\n",
    "            if word in doc.lower():\n",
    "                if (y[i] == 1):\n",
    "                    pos_count += 1\n",
    "                else:\n",
    "                    neg_count += 1\n",
    "                    \n",
    "        if pos_count > neg_count:\n",
    "            connotation[word] = 1\n",
    "        else:\n",
    "            connotation[word] = 0\n",
    "    \n",
    "    return word_list, connotation\n",
    "\n",
    "def generate_appearance(X_train_corpus, X_test_corpus, word_list, connotation):\n",
    "    y_train_agreement = []\n",
    "    for i in range(len(X_train_corpus)):\n",
    "        doc_agreement = []\n",
    "        for word in word_list:\n",
    "            if word in X_train_corpus[i]:\n",
    "                if connotation[word] == 1:\n",
    "                    doc_agreement.append(1)\n",
    "                else:\n",
    "                    doc_agreement.append(-1)\n",
    "            else:\n",
    "                doc_agreement.append(0)\n",
    "        y_train_agreement.append(doc_agreement)\n",
    "        \n",
    "    y_test_agreement = []\n",
    "    for i in range(len(X_test_corpus)):\n",
    "        doc_agreement = []\n",
    "        for word in word_list:\n",
    "            if word in X_test_corpus[i]:\n",
    "                if connotation[word] == 1:\n",
    "                    doc_agreement.append(1)\n",
    "                else:\n",
    "                    doc_agreement.append(-1)\n",
    "            else:\n",
    "                doc_agreement.append(0)\n",
    "        y_test_agreement.append(doc_agreement)\n",
    "        \n",
    "    return np.array(y_train_agreement), np.array(y_test_agreement)\n",
    "\n",
    "# 'imdb-unigrams.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now try on IMDB data\n",
    "#### Like the real IMDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "# make sure that the first shape is the IMDB training data. \n",
    "\n",
    "def open_pickle(path):\n",
    "    import pickle\n",
    "    with open(path, 'rb') as f:\n",
    "        X = pickle.load(f)\n",
    "    return X\n",
    "\n",
    "X_train_original = open_pickle('../data/imdb/imdb_original_preprocessed_xtrain.pickle')\n",
    "X_test_original = open_pickle('../data/imdb/imdb_original_preprocessed_xtest.pickle')\n",
    "y_train_original = open_pickle('../data/imdb/imdb_original_preprocessed_ytrain.pickle')\n",
    "y_test_original = open_pickle('../data/imdb/imdb_original_preprocessed_ytest.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count vectorizer \n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "cv = CountVectorizer(min_df = 100)\n",
    "X_train = cv.fit_transform(X_train_original)\n",
    "X_test = cv.transform(X_test_original)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_list, connotation = load_unigrams('./imdb-unigrams.txt', X_train_original, y_train_original)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_agreement, y_test_agreement = generate_appearance(X_train_original, X_test_original, \n",
    "                                                          word_list, connotation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_agreement[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(word_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build the combined model\n",
    "# Combined model\n",
    "human_terms_len = len(word_list)\n",
    "\n",
    "base_model = build_base_model(X_train.shape[1])\n",
    "\n",
    "combined_input_layer = Input(shape=(X_train.shape[1],))\n",
    "\n",
    "# build the hard coded weight for human terms\n",
    "ht_input_layer = Input(shape=(human_terms_len,))\n",
    "\n",
    "split = Lambda( lambda x: tf.split(x,num_or_size_splits=human_terms_len,axis=1))(ht_input_layer)\n",
    "\n",
    "# get the document prediction\n",
    "label_layer = base_model(combined_input_layer)\n",
    "\n",
    "# multiply and pass it into relu\n",
    "# initialize relu layer\n",
    "\n",
    "relu_layer = Dense(1, activation='relu', name='relu_layer', use_bias=False, kernel_initializer='ones')\n",
    "\n",
    "# stack the multiply layer\n",
    "dense_layer = []\n",
    "for i in range(human_terms_len):\n",
    "    dense_layer.append(relu_layer(Multiply()([split[i], label_layer])))\n",
    "\n",
    "# concat all the result   \n",
    "concat = Lambda( lambda x: tf.concat(x, axis=1), name='concatenate')(dense_layer)\n",
    "\n",
    "# pass it to sigmoid layer\n",
    "output_layer = Dense(1, activation='sigmoid')(concat)\n",
    "\n",
    "combined_model = Model(inputs=[combined_input_layer, ht_input_layer], outputs=output_layer)\n",
    "combined_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model.compile(loss='mse',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['acc'])\n",
    "\n",
    "combined_model.compile(loss='mse',\n",
    "                      optimizer='adam',\n",
    "                      metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_tanh = y_train_original\n",
    "y_train_tanh[y_train_tanh == 0] = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model.fit(X_train, y_train_tanh, batch_size=1, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model.fit(X_train, y_train_tanh, batch_size=1, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_model.fit([X_train,\n",
    "                   y_train_agreement])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
