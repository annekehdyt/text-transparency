{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(75, 2)\n",
      "corpus update start\n",
      "corpus update end\n",
      "\n",
      "(75, 2)\n",
      "corpus update start\n",
      "corpus update end\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "from dataset_load import *\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "path = r'../../data/womens-ecommerce-clothing-reviews/Womens_Clothing_E-Commerce_Reviews.csv'\n",
    "\n",
    "df = pd.read_csv(path)\n",
    "\n",
    "X = list(df['Review Text'])\n",
    "y = list(df['Rating'])\n",
    "y_label = np.asarray(y)\n",
    "\n",
    "\n",
    "y_label[y_label<3] = 0\n",
    "y_label[y_label>3] = 1\n",
    "\n",
    "neutral_indices = np.where(y_label==3)[0]\n",
    "y_label = np.delete(y_label, neutral_indices)\n",
    "X = np.delete(X, neutral_indices)\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split, ShuffleSplit\n",
    "\n",
    "# split\n",
    "X_train_split, X_test_split, y_train, y_test = train_test_split(X, y_label, test_size=0.33, random_state=42)\n",
    "\n",
    "# preprocessing\n",
    "X_train_corpus_update = update_corpus_contraction(X_train_split)\n",
    "X_test_corpus_update = update_corpus_contraction(X_test_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################################################\n",
    "\n",
    "# Count vectorizer \n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "token = r\"(?u)\\b[\\w\\'/]+\\b\"\n",
    "cv = CountVectorizer(min_df = 100, token_pattern=token, lowercase=True, binary=True)\n",
    "X_train = cv.fit_transform(X_train_corpus_update)\n",
    "X_test = cv.transform(X_test_corpus_update)\n",
    "\n",
    "######################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "720"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(cv.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LR1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l1', random_state=42, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "lr1 = LogisticRegression(penalty='l1', random_state=42)\n",
    "lr1.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LR2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_list, connotation = load_unigrams('./ecom-unigrams.txt', X_train_corpus_update, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_agreement, y_test_agreement = generate_appearance(X_train_corpus_update, X_test_corpus_update, \n",
    "                                                          word_list, connotation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "htcv = CountVectorizer(min_df = 100, token_pattern=token, lowercase=True, binary=True, vocabulary=word_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "ht_train = htcv.transform(X_train_corpus_update)\n",
    "ht_test = htcv.transform(X_test_corpus_update)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_agreement_pos = np.copy(y_train_agreement)\n",
    "y_train_agreement_neg = np.copy(y_train_agreement)\n",
    "y_test_agreement_pos = np.copy(y_test_agreement)\n",
    "y_test_agreement_neg = np.copy(y_test_agreement)\n",
    "\n",
    "y_train_agreement_pos[y_train_agreement_pos == -1] = 0\n",
    "y_train_agreement_neg[y_train_agreement_neg == 1] = 0\n",
    "y_test_agreement_pos[y_test_agreement_pos == -1] = 0\n",
    "y_test_agreement_neg[y_test_agreement_neg == 1] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### From actual label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr2_train = list()\n",
    "for i,y in enumerate(y_train):\n",
    "    if y == 1:\n",
    "        lr2_train.append(y_train_agreement_pos[i] * y)\n",
    "    else:\n",
    "        lr2_train.append(y_train_agreement_neg[i] * y)\n",
    "        \n",
    "lr2_train = np.asarray(lr2_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l1', random_state=42, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr2 = LogisticRegression(penalty='l1', random_state=42)\n",
    "lr2.fit(lr2_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8821108334558283\n",
      "0.8821108334558283\n",
      "0.8837966985230234\n"
     ]
    }
   ],
   "source": [
    "# Test with predict {0,1}\n",
    "\n",
    "lr1_predict = lr1.predict(X_test)\n",
    "\n",
    "lr2_test_bin = list()\n",
    "for i,y in enumerate(lr1_predict):\n",
    "    if y==1:\n",
    "        lr2_test_bin.append(y_test_agreement_pos[i] * y)\n",
    "    else:\n",
    "        lr2_test_bin.append(y_test_agreement_neg[i] * y)\n",
    "    \n",
    "lr2_test_bin = np.asarray(lr2_test_bin)\n",
    "\n",
    "print(lr2.score(lr2_test_bin, y_test))\n",
    "\n",
    "# Test with predict [0,1]\n",
    "\n",
    "lr1_predict = lr1.predict_proba(X_test)[:,1]\n",
    "\n",
    "lr2_test_proba = list()\n",
    "for i,y in enumerate(lr1_predict):\n",
    "    if y>=0.5:\n",
    "        lr2_test_proba.append(y_test_agreement_pos[i] * y)\n",
    "    else:\n",
    "        lr2_test_proba.append(y_test_agreement_neg[i] * y)\n",
    "    \n",
    "lr2_test_proba = np.asarray(lr2_test_proba)\n",
    "\n",
    "print(lr2.score(lr2_test_proba, y_test))\n",
    "print(lr2.score(lr2_train,y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2.07225276, 2.26577944, 2.17978121, 3.36649573, 3.63447905,\n",
       "       5.39642824, 3.06199704, 3.67335775, 4.12306176, 2.01960772,\n",
       "       2.17205167, 2.55062902, 4.96199048, 3.05388252, 5.87407455,\n",
       "       4.16544223, 3.35259895, 2.16041677, 0.        , 0.        ,\n",
       "       0.        , 0.        , 2.08388897, 1.50680413, 0.        ,\n",
       "       2.67218354, 2.84222275, 2.67236185, 1.42215374, 0.67960568,\n",
       "       1.39062592, 4.53589323, 2.60412104, 6.15349448, 1.96403106,\n",
       "       2.19517056, 1.23346016, 5.3185532 , 5.38501   , 4.00282141,\n",
       "       3.77793913, 2.51987386, 1.98087521, 2.95247283, 3.04782471,\n",
       "       3.41367959, 3.07789941, 2.31556245, 2.85939093, 4.16083684,\n",
       "       5.20250887, 3.48543535])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weight = lr2.coef_.flatten()\n",
    "weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_weight(clf1, clf2=None):\n",
    "    w = clf1.coef_.flatten()\n",
    "    indices = np.argsort(w)[::-1]\n",
    "    con = list(connotation.values())\n",
    "\n",
    "    if clf2 is not None:\n",
    "        w2 = clf2.coef_.flatten()\n",
    "        for i in indices:\n",
    "            print('%s \\t %.3f \\t %.3f \\t %d' %(word_list[i], w[i], w2[i], con[i]))\n",
    "    else:  \n",
    "        for i in indices:\n",
    "            print('%s \\t %.3f \\t %d' %(word_list[i], w[i],con[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train with predicted label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_y = lr1.predict(X_train)\n",
    "proba_y = lr1.predict_proba(X_train)[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr2_train_bin = list()\n",
    "for i,y in enumerate(predicted_y):\n",
    "    if y == 1:\n",
    "        lr2_train_bin.append(y_train_agreement_pos[i] * y)\n",
    "    else:\n",
    "        lr2_train_bin.append(y_train_agreement_neg[i] * y)\n",
    "        \n",
    "lr2_train_bin = np.asarray(lr2_train_bin)\n",
    "\n",
    "lr2_train_proba = list()\n",
    "for i,y in enumerate(proba_y):\n",
    "    if y >= 0.5:\n",
    "        lr2_train_proba.append(y_train_agreement_pos[i] * y)\n",
    "    else:\n",
    "        lr2_train_proba.append(y_train_agreement_neg[i] * y)\n",
    "        \n",
    "lr2_train_proba = np.asarray(lr2_train_proba)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l1', random_state=42, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train with {0,1} label\n",
    "# Train with [0,1] proba\n",
    "\n",
    "lr2_y_hat_bin = LogisticRegression(penalty='l1', random_state=42)\n",
    "lr2_y_hat_proba = LogisticRegression(penalty='l1', random_state=42)\n",
    "\n",
    "lr2_y_hat_bin.fit(lr2_train_bin, y_train)\n",
    "lr2_y_hat_proba.fit(lr2_train_proba, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_pred_bin = lr2_y_hat_bin.predict(lr2_test_bin)\n",
    "y_test_pred_proba = lr2_y_hat_proba.predict(lr2_test_proba)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8821108334558283\n",
      "0.8869616345729825\n",
      "\n",
      "0.8837966985230234\n",
      "0.8905299739357081\n"
     ]
    }
   ],
   "source": [
    "print(lr2_y_hat_bin.score(lr2_test_bin,y_test))\n",
    "print(lr2_y_hat_proba.score(lr2_test_proba,y_test))\n",
    "print()\n",
    "print(lr2_y_hat_bin.score(lr2_train_bin,y_train))\n",
    "print(lr2_y_hat_proba.score(lr2_train_proba,y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.57301472]\n",
      "[0.5918641]\n"
     ]
    }
   ],
   "source": [
    "print(lr2_y_hat_bin.intercept_)\n",
    "print(lr2_y_hat_proba.intercept_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print_weight(lr2_y_hat_bin, lr2_y_hat_proba)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print_weight(lr2_y_hat_proba)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What if... we put scale and ReLU manually\n",
    "\n",
    "If we do not scale. Then, the negative weights would be closer to zero. The magnitude would be lesser."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_y = lr1.predict(X_train)\n",
    "proba_y = lr1.predict_proba(X_train)[:,1]\n",
    "\n",
    "# Scale\n",
    "\n",
    "predicted_y = predicted_y * 2 - 1\n",
    "proba_y = proba_y * 2 - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr2_train_bin = list()\n",
    "for i,y in enumerate(predicted_y):\n",
    "    if y == 1:\n",
    "        lr2_train_bin.append(y_train_agreement_pos[i] * y)\n",
    "    else:\n",
    "        lr2_train_bin.append(y_train_agreement_neg[i] * y)\n",
    "        \n",
    "lr2_train_bin = np.asarray(lr2_train_bin)\n",
    "\n",
    "lr2_train_proba = list()\n",
    "for i,y in enumerate(proba_y):\n",
    "    if y >= 0.5:\n",
    "        lr2_train_proba.append(y_train_agreement_pos[i] * y)\n",
    "    else:\n",
    "        lr2_train_proba.append(y_train_agreement_neg[i] * y)\n",
    "        \n",
    "lr2_train_proba = np.asarray(lr2_train_proba)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l1', random_state=42, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train with {0,1} label\n",
    "# Train with [0,1] proba\n",
    "\n",
    "lr2_y_hat_bin = LogisticRegression(penalty='l1', random_state=42)\n",
    "lr2_y_hat_proba = LogisticRegression(penalty='l1', random_state=42)\n",
    "\n",
    "lr2_y_hat_bin.fit(lr2_train_bin, y_train)\n",
    "lr2_y_hat_proba.fit(lr2_train_proba, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with predict {0,1}\n",
    "\n",
    "predicted_y_test = lr1.predict(X_test)\n",
    "predicted_y_test = predicted_y_test * 2 - 1\n",
    "\n",
    "lr2_test_bin = list()\n",
    "for i,y in enumerate(predicted_y_test):\n",
    "    if y==1:\n",
    "        lr2_test_bin.append(y_test_agreement_pos[i] * y)\n",
    "    else:\n",
    "        lr2_test_bin.append(y_test_agreement_neg[i] * y)\n",
    "    \n",
    "lr2_test_bin = np.asarray(lr2_test_bin)\n",
    "\n",
    "\n",
    "# Test with predict [0,1]\n",
    "\n",
    "predicted_y_test_proba = lr1.predict_proba(X_test)[:,1]\n",
    "predicted_y_test_proba = predicted_y_test_proba * 2 - 1\n",
    "\n",
    "lr2_test_proba = list()\n",
    "for i,y in enumerate(predicted_y_test_proba):\n",
    "    if y>=0:\n",
    "        lr2_test_proba.append(y_test_agreement_pos[i] * y)\n",
    "    else:\n",
    "        lr2_test_proba.append(y_test_agreement_neg[i] * y)\n",
    "    \n",
    "lr2_test_proba = np.asarray(lr2_test_proba)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9021020138174335\n",
      "0.9015140379244451\n",
      "\n",
      "0.9103677961193165\n",
      "0.9112366058499856\n"
     ]
    }
   ],
   "source": [
    "print(lr2_y_hat_bin.score(lr2_test_bin,y_test))\n",
    "print(lr2_y_hat_proba.score(lr2_test_proba,y_test))\n",
    "print()\n",
    "print(lr2_y_hat_bin.score(lr2_train_bin,y_train))\n",
    "print(lr2_y_hat_proba.score(lr2_train_proba,y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "accept_indices = np.where(np.sum(lr2_test_proba,axis=1)!=0)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9530994364660971\n",
      "0.9523722959461915\n"
     ]
    }
   ],
   "source": [
    "print(lr2_y_hat_bin.score(lr2_test_bin[accept_indices], y_test[accept_indices]))\n",
    "print(lr2_y_hat_proba.score(lr2_test_proba[accept_indices], y_test[accept_indices]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([   3,   12,   24, ..., 6776, 6788, 6792], dtype=int64)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.where(np.sum(lr2_test_bin,axis=1)==0)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.19138615316772012"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(np.sum(lr2_test_proba,axis=1)==0)/lr2_test_proba.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1302"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(np.sum(lr2_test_proba,axis=1)==0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.99999879, 0.        , 0.99999879, 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.99999879, 0.        , 0.99999879,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.99999879, 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.99999879])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr2_test_proba[8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 1])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr2_test_bin[8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.999998786730796"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_y_test_proba[8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_y_test[8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "subtle \t 2.198 \t 2.021 \t 1\n",
      "versatile \t 2.111 \t 2.348 \t 1\n",
      "chic \t 2.081 \t 1.798 \t 1\n",
      "glad \t 2.043 \t 2.057 \t 1\n",
      "pleased \t 2.000 \t 1.774 \t 1\n",
      "perfect \t 1.935 \t 2.575 \t 1\n",
      "comfy \t 1.800 \t 2.192 \t 1\n",
      "comfortable \t 1.668 \t 2.245 \t 1\n",
      "complaint \t 1.615 \t 2.065 \t 1\n",
      "stylish \t 1.574 \t 1.515 \t 1\n",
      "elegant \t 1.440 \t 1.426 \t 1\n",
      "great \t 1.418 \t 2.018 \t 1\n",
      "stunning \t 1.406 \t 2.290 \t 1\n",
      "happy \t 1.400 \t 1.682 \t 1\n",
      "excellent \t 1.335 \t 1.184 \t 1\n",
      "fits \t 1.328 \t 1.681 \t 1\n",
      "amazing \t 1.308 \t 1.524 \t 1\n",
      "fun \t 1.291 \t 1.908 \t 1\n",
      "simple \t 1.260 \t 1.666 \t 1\n",
      "roomy \t 1.249 \t 2.365 \t 1\n",
      "feminine \t 1.246 \t 1.431 \t 1\n",
      "hits \t 1.199 \t 1.755 \t 1\n",
      "soft \t 1.164 \t 1.799 \t 1\n",
      "loose \t 1.123 \t 1.753 \t 1\n",
      "worried \t 1.088 \t 1.417 \t 1\n",
      "cozy \t 1.072 \t 1.311 \t 1\n",
      "fabulous \t 1.056 \t 1.981 \t 1\n",
      "beautiful \t 1.030 \t 1.794 \t 1\n",
      "nice \t 0.966 \t 1.917 \t 1\n",
      "gorgeous \t 0.949 \t 1.532 \t 1\n",
      "best \t 0.941 \t 1.775 \t 1\n",
      "recommend \t 0.928 \t 1.758 \t 1\n",
      "flattering \t 0.915 \t 1.953 \t 1\n",
      "lovely \t 0.740 \t 1.274 \t 1\n",
      "fantastic \t 0.573 \t 1.430 \t 1\n",
      "bigger \t 0.214 \t 0.792 \t 1\n",
      "oversized \t 0.000 \t 0.491 \t 1\n",
      "itchy \t 0.000 \t 0.000 \t 1\n",
      "perfectly \t -0.031 \t -0.282 \t 1\n",
      "returned \t -0.043 \t 0.507 \t 1\n",
      "excited \t -0.140 \t 0.274 \t 1\n",
      "dry \t -0.170 \t 0.865 \t 1\n",
      "problem \t -0.234 \t 0.615 \t 1\n",
      "returning \t -0.272 \t 0.285 \t 1\n",
      "nothing \t -0.293 \t 0.063 \t 1\n",
      "sadly \t -0.677 \t -0.648 \t 1\n",
      "bad \t -0.912 \t 0.016 \t 1\n",
      "weird \t -1.084 \t -0.800 \t 1\n",
      "huge \t -2.512 \t -3.555 \t 0\n",
      "disappointed \t -2.598 \t -3.771 \t 0\n",
      "unflattering \t -2.732 \t -3.576 \t 0\n",
      "cheap \t -2.823 \t -4.271 \t 0\n"
     ]
    }
   ],
   "source": [
    "print_weight(lr2_y_hat_bin, lr2_y_hat_proba)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# becasue LR sklearn use a regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
